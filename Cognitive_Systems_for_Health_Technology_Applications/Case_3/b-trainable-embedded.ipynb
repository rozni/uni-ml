{
  "cells": [
    {
      "metadata": {
        "_uuid": "33150c1de0d64d22fc111ae0487570fd3c14a7fe",
        "trusted": true
      },
      "cell_type": "code",
      "source": "VOCABULARY_SIZE = 4095\nVALIDATION_SPLIT = 0.25",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# basic imports\nimport os\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport keras\nfrom keras import layers\n\nfrom sklearn.model_selection import train_test_split",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7b90fe650019cf2a3e554b85343c9063580d1cb6",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# a NLP library\n# get pretrained model with embeddings and load\n\n# if GPU is on, Internet connection is needed in Kaggle Kernels!\n!python -m spacy download en_core_web_lg\n\nimport spacy\nnlp = spacy.load('en_core_web_lg')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "601ee1725169ff092b315c6681657c221f823781",
        "trusted": true
      },
      "cell_type": "code",
      "source": "### plotting ###\n# prettier and larger basic graphs\nsns.set(rc={\n    'figure.figsize':(18,8),\n    'axes.titlesize':14,\n})\n\n### pandas ###\n# no need to see many decimal places and makes nicer horizontal fits :-)\npd.options.display.float_format = '{:.3f}'.format\n# pd.options.display.precision = 3\n# make the tables more compact vertically, too\npd.options.display.max_rows = 20\n\n### numpy ###\n# same as for pandas - max. 3 decimal places\nnp.set_printoptions(formatter={'float_kind':'{:.3f}'.format})\n# np.set_printoptions(precision=3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(os.listdir(\"../input\"))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "reviews_df = pd.read_csv('../input/drugsComTrain_raw.csv')\nreviews_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "reviews_test_df = pd.read_csv('../input/drugsComTest_raw.csv')\nreviews_test_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0387c3de93f63d19d9f16a529b42e92e9c88a660"
      },
      "cell_type": "markdown",
      "source": "The reviews are quite raw (containing unhealthy things for tokenizers such as `&#039;` *- an HTML code for apostrophe*),\nI will quickly cook them a bit before progressing further..."
    },
    {
      "metadata": {
        "_uuid": "8a38176c3a93d028cb73eec4a1e284694aeaa313",
        "trusted": true
      },
      "cell_type": "code",
      "source": "## sanitizing review text\n\nfrom html import unescape\n\ndef clean_review(text):\n    \"\"\"Replace HTML escaped characters\n    and strip outer quotes and spaces\"\"\"\n    return unescape(text.strip(' \"\\'')).replace('\\r', ' ').replace('\\n', ' ').replace('\\ufeff1', '')\n\nreviews_df.review = reviews_df.review.apply(clean_review)\nreviews_test_df.review = reviews_test_df.review.apply(clean_review)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4467ab8975c57878a46431c31fe9d2aa20352865"
      },
      "cell_type": "markdown",
      "source": "Today we have a soup made mostly with the following ingredients"
    },
    {
      "metadata": {
        "_uuid": "90974c9781e0399d81b6d430334023af7996aea6",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from wordcloud import WordCloud\n\nwordcloud = WordCloud(\n    width=1000,\n    height=500,\n    random_state=hash('yummy'),\n    max_font_size=110\n)\nwordcloud.generate('\\n'.join(reviews_df.review))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()\ndel wordcloud",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8a38176c3a93d028cb73eec4a1e284694aeaa313",
        "trusted": true
      },
      "cell_type": "code",
      "source": "## lemmatizing review text\n\ndef text2lexemes(text):\n    \"\"\"tokenize -> lemmatize\"\"\"\n    \n    tokens = nlp(\n        # also split on \"/\"\n        text.replace('/', ' / '),\n        # we only need tokenizer and lemmas, so disable the rest\n        disable=['tagger', 'parser', 'ner']\n    )\n    \n    lexemes = []\n    for token in tokens:\n        \n        if nlp.vocab[token.lemma_.lower()].has_vector:\n            lexeme = token.lemma_.lower()\n        elif nlp.vocab[token.norm_.lower()].has_vector:\n            lexeme = token.norm_.lower()\n        else:\n            lexeme = token.lower_\n        \n        lexemes.append(lexeme)\n        \n    return lexemes\n\nreviews_word_seq = [text2lexemes(review) for review in reviews_df.review]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "def48a44d055278e2a0af144bc4b3d0e46494f18"
      },
      "cell_type": "code",
      "source": "# vocabulary map\n# count occurences of each word\nword_count = {}\nfor lemmas in reviews_word_seq:\n    for lemma in lemmas:\n        word_count[lemma] = word_count.get(lemma, 0) + 1\nword_count = dict(sorted(word_count.items(), key=lambda pair: pair[1], reverse=True))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "62180d23315fac371bf833c8319c7493d390265d"
      },
      "cell_type": "code",
      "source": "word_count",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cf3883b8c4f66f8d4f8acd0ebb0ef44cbde9033d"
      },
      "cell_type": "code",
      "source": "## word sequence to integer sequences\n\nvocab = list(word_count)[:VOCABULARY_SIZE]\nword2index = {word:index for index, word in enumerate(vocab, start=1)}\n\nreviews_seq = [\n    [\n        word2index.get(word, 0)\n        for word in review\n        # dismiss out-of-vocabulary words\n        if word in word2index\n    ]\n    for review in reviews_word_seq\n]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ec83e3274e6197846230a67f8565a1b7be4789db"
      },
      "cell_type": "code",
      "source": "plt.title('Vocabulary Distribution')\n\nwc_vals = list(word_count.values())\n\n# plt.plot(vf, '.', label='Words how they came')\nplt.plot(wc_vals, 'o', label='Lexeme count')\nplt.plot(sum(wc_vals)-np.cumsum(wc_vals), '.', label='Uncaptured words')\nplt.axvline(x=VOCABULARY_SIZE, color='yellowgreen', label=f'Current vocabulary size ({VOCABULARY_SIZE})')\n\nplt.gca().set_yscale('log')\n# plt.gca().set_xscale('log')\n\nplt.legend()\nplt.xlabel('Vocabulary Size')\nplt.ylabel('Amount')\n\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d9900d13fea47a0d948492e97b34202d5fdda9eb",
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.title('Distribution of Review Lengths')\n\nreview_lengths = np.array([len(s) for s in reviews_seq])\n\nsequence_cutoff_legth = int(np.quantile(review_lengths, 0.99))\n\nsns.distplot(\n    review_lengths,\n    hist_kws=dict(label='Normalized histogram'),\n    kde=True,\n    kde_kws=dict(label='Kernel density'),\n    rug=True,\n    norm_hist=False,\n    rug_kws=dict(color='orangered', label='Review'),\n    axlabel='Sequence Length',\n)\nplt.axvline(\n    x=sequence_cutoff_legth,\n    color='yellowgreen',\n    label=f'Sequence cutoff length ({sequence_cutoff_legth})'\n)\n\nplt.xlabel('Review Length')\nplt.ylabel('Density')\nplt.legend();",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "722bc42ef7151da5c3e4ef562e2f96b67072ed95",
        "trusted": true
      },
      "cell_type": "code",
      "source": "## pad the sequences\n\nreviews_seq = keras.preprocessing.sequence.pad_sequences(\n    reviews_seq,\n    maxlen=sequence_cutoff_legth\n)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ba0837ffd8ca56f74eb82d01efaa15ee742441bf",
        "trusted": true
      },
      "cell_type": "code",
      "source": "## scale values\n\n# range will be 0.1 - 1.0\n# very easy interpretation for the MAE metrics\nreviews_df.rating /= 10\n\n# standard scaling\nreviews_df.usefulCount += reviews_df.usefulCount.mean()\nreviews_df.usefulCount /= reviews_df.usefulCount.std()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bdef422dbe24af8d54074b01ee11999503cb868a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# plotting\n\ndef plot_history(history, skip_first_n_epochs=0):\n    \"\"\"Show information about the training\"\"\"\n    \n    # plot every train-valid metric pair separately\n    for metric in history:\n        if not metric.startswith('val_'):\n            x = np.arange(len(history[metric]))+1\n\n            y_train = history[metric][skip_first_n_epochs:]\n            y_valid = history['val_'+metric][skip_first_n_epochs:]\n\n            # make the plots\n            plt.plot(x, y_train)\n            plt.plot(x, y_valid)\n\n            # add a legend\n            plt.legend([metric, 'val_'+metric], fontsize='large')\n\n            # title with min/max stats\n            plt.title(\n                f'{metric.upper()} - '\n                f'min/max [train: {min(y_train):.3f}/{max(y_train):.3f}, '\n                f'valid: {min(y_valid):.3f}/{max(y_valid):.3f}]'\n            )\n            \n            # label and show\n            plt.xlabel('epoch')\n            plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "72335a45295fc84ab7b316ca4cb8214bed2824db"
      },
      "cell_type": "code",
      "source": "embedding_weights = np.zeros((\n    VOCABULARY_SIZE+1, # indices/hashes\n    nlp.vocab.vectors_length # embedding dimmension\n))\nfor word, index in word2index.items():\n    embedding_weights[index] = nlp.vocab[word].vector",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "efc79e3b090b5d46bb5314472bc386dc3219451e"
      },
      "cell_type": "code",
      "source": "input_reviews = layers.Input(shape=(sequence_cutoff_legth,), dtype='int32')\n\nbranch_b = layers.Embedding(\n    *embedding_weights.shape,\n    weights=[embedding_weights],\n    input_length=sequence_cutoff_legth,\n    trainable=True,\n)(input_reviews)\nbranch_b = layers.GaussianNoise(0.05)(branch_b)\nbranch_b = layers.Conv1D(16, 3, padding='same', activation='relu')(branch_b)\nbranch_b = layers.BatchNormalization()(branch_b)\nbranch_b = layers.MaxPool1D(2)(branch_b)\n\nbranch_b = layers.Conv1D(32, 3, padding='same', activation='relu')(branch_b)\nbranch_b = layers.BatchNormalization()(branch_b)\nbranch_b = layers.MaxPool1D(2)(branch_b)\n\nbranch_b = layers.Conv1D(64, 3, padding='same', activation='relu')(branch_b)\nbranch_b = layers.BatchNormalization()(branch_b)\nbranch_b = layers.MaxPool1D(2)(branch_b)\n\nbranch_b = layers.Conv1D(128, 3, padding='same', activation='relu')(branch_b)\nbranch_b = layers.BatchNormalization()(branch_b)\nbranch_b = layers.GlobalMaxPool1D()(branch_b)\n\nbranch_b = layers.Dense(10, activation='relu')(branch_b)\nbranch_b = layers.BatchNormalization()(branch_b)\nbranch_b = layers.Dropout(0.1)(branch_b)\n\n\ninput_useful_count = layers.Input(shape=(1,))\n\n\nmodel_top = layers.concatenate([branch_b, input_useful_count])\nmodel_top = layers.Dense(10, activation='relu')(model_top)\nmodel_top = layers.BatchNormalization()(model_top)\nmodel_output = layers.Dense(1, activation='sigmoid')(model_top)\n\nmodel = keras.models.Model(\n    inputs=[input_reviews, input_useful_count],\n    outputs=model_output,\n)\nmodel.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "af7852931cc081c5a50c190092014779ad0eb7d6"
      },
      "cell_type": "code",
      "source": "(\n    y_train, y_valid,\n    x1_train, x1_valid,\n    x2_train, x2_valid,\n) = train_test_split(\n    reviews_df.rating.values, # y\n    reviews_seq,\n    reviews_df.usefulCount.values,\n    # options\n    test_size=VALIDATION_SPLIT,\n    stratify=reviews_df.rating.values\n)\nx_train = [x1_train, x2_train]\nx_valid = [x1_valid, x2_valid]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "043aec47574c1e62321af572b71f163a3afd795c",
        "trusted": true
      },
      "cell_type": "code",
      "source": "model.compile(\n    optimizer='adam',\n    loss='mae',\n)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "db91fa575c01c026ca1547237960fa1125fa3d95",
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "history = model.fit(\n    x=x_train,\n    y=y_train,\n    validation_data=(x_valid, y_valid),\n    batch_size=256,\n    epochs=120,\n    verbose=1,\n    callbacks=[\n        keras.callbacks.ModelCheckpoint(\n            'model-epoch_{epoch:02d}-val_loss_{val_loss:.2f}.hdf5',\n            monitor='val_loss',\n            verbose=0,\n            save_best_only=True,\n            save_weights_only=False,\n            mode='auto',\n            period=1,\n        ),\n    ],\n)\n\nplot_history(history.history)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f057889e92b2f393518ed9e231f265cbb26e94ed"
      },
      "cell_type": "code",
      "source": "x = [np.concatenate(z) for z in zip(x_train, x_valid)]\ny = np.concatenate([y_train, y_valid])\n\ncolor = ['#FFC07DA0']*len(y_train) + ['#988F32A0']*len(y_valid) # A276DD, 7347AD\ny, (*x), color = zip(*sorted(zip(y, *x, color), key=lambda pair: pair[0]))\nx = [np.asarray(i) for i in x]\n\nyh = model.predict(x).ravel()\n\nplt.scatter(range(len(y)), yh, c=color)\nplt.scatter(range(len(y)), y, c='#73DA4D')",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}