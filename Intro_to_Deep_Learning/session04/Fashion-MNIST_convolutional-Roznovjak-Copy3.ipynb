{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For designing the models I split the training dataset into 50,000 samples of data for training and 10,000 for validation. All accuracy values here are based on the validation part, except for the final model.\n",
    "\n",
    "At the end of the design phase I also began experimenting with data augmentation - horizontal flips (which I have been considering not to use on shoes for example after examining the dataset - they seem to be always only in one direction, but I have not tried it in the end) and various forms of simple data normalization.\n",
    "\n",
    "I started with a very basic model in order to have some baseline and seeing how the training goes. It reached 90% accuracy - pretty promissing for just a 5C3-2P2-64-10, given that the highest accuracy I got using deep, dense models was 89% (using whole training set for training and the test set for validation).\n",
    "\n",
    "The training was taking just a moment so I was experimenting with hyperparameters of the basic model - number of filters, padding, kernel size, etc. Then I made the models deeper and was trying to get a 'feel' for their behaviour. I wanted to see how LeNet-5 would perform on this dataset - I found out it virtually could not get over 90% accuracy, I looked up for a different LeNet model, I coppied one from [this article](https://www.pyimagesearch.com/2016/08/01/lenet-convolutional-neural-network-in-python/) it was reaching 91.5%.\n",
    "\n",
    "I found out that narrowing the model too much by doing many convolutions without padding hungers the model of parameters and renders it underfitting - it seems that models with less then 100,000 weights were hungry and could not get above 93% accuracy.\n",
    "\n",
    "The other problem was trying to tame the overfitting models, of course, I was thinking about regularization but I had wasn't sure whether it is a good idea (as there are not so many parameters and the filters work differently than dense layers), I tried a couple of examples and I would say it was inconslusive - sometimes very good results, sometimes severe underfitting (especially with L2 weight regularization)...  I came across [this](https://arxiv.org/pdf/1512.01400.pdf) paper which gave me some direction where to apply dropout layers and what does it represent. The technique of putting a dropout layer before max-pooling increased the accuracy of, for example, the LeNet model in previous paragraph to 92.5% without tuning. I also tried putting a dropout layer on the very input layer, in faith that it would do some sort of data diversification/augmentation, however if there were already other regularizers introduced, the effect was a considerably slower learning rate and not much benefit in terms of accuracy.\n",
    "\n",
    "In the end I iterated through to models which were reaching almost 94% accuracy without regularization (particulary one containing about 0.62M parameters and another having about 0.23M), I decided to fine-tune and train using augmented data the bigger model. Its final accuracy is {}% which puts it on a surprisingly good place among the submitted benchmarks in [Fashion-MNIST's GitHub repo](https://github.com/zalandoresearch/fashion-mnist).\n",
    "\n",
    "I have been pondering what results would the following process bring: \n",
    "1. training model with little or no hidden dense layers, several convolutional and pooling layers\n",
    "2. disabling training on these layers afterwards\n",
    "3. replacing the output layer with a submodel of dense layers\n",
    "4. training the model again (the added layers)\n",
    "\n",
    "I suspect, this way better filters or something interesting might happen, unfortunatelly, I did not know how to achieve this simply...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# for switching between GPU and CPU\n",
    "NO_GPU = False\n",
    "\n",
    "if NO_GPU:\n",
    "    import os\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import keras.layers as kl\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(*layers, verbose=False,\n",
    "               optimizer='adam', loss='categorical_crossentropy', metrics=['acc'],\n",
    "               compile_kwargs={}):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    for layer in layers:\n",
    "        model.add(layer)\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"Model summary:\")\n",
    "        model.summary()\n",
    "    \n",
    "    for kw in ('optimizer', 'loss', 'metrics'):\n",
    "        if not kw in compile_kwargs:\n",
    "            compile_kwargs[kw] = locals()[kw]\n",
    "    model.compile(**compile_kwargs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, figsize=(15,4), title='', columns=2, start_x_from=0):\n",
    "    \"\"\"Graphs a history for each key (combines validation and training keys into one plot).\n",
    "    \n",
    "    start_x_from=N skips the first N entries.\n",
    "    \n",
    "    History can be a whole training history class or just a dict.\"\"\"\n",
    "    \n",
    "    if hasattr(history, 'history'): # full history given\n",
    "        history = history.history   # only that history is enough\n",
    "        \n",
    "    assert hasattr(history, 'keys')\n",
    "    keys = [key for key in history.keys() if not key.startswith(\"val_\")]\n",
    "    assert keys # there is one at least\n",
    "    epochs = list(range(1,len(history[keys[0]])+1)) # all should have the same size list\n",
    "    \n",
    "    rows = np.ceil(len(keys)/columns).astype('int')\n",
    "    \n",
    "    fig=plt.figure(figsize=figsize)\n",
    "    f = plt.title(title)\n",
    "    f.axes.get_xaxis().set_visible(False)\n",
    "    f.axes.get_yaxis().set_visible(False)\n",
    "    \n",
    "    i = 1\n",
    "    for key in sorted(keys):\n",
    "        valkey = \"val_\" + key\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        i += 1\n",
    "        plt.plot(epochs[start_x_from:], history[key][start_x_from:], label=\"Training \" + key,\n",
    "                 marker='.', color='#00A287', linestyle='')\n",
    "        \n",
    "        late_avg = np.mean(history[key][(len(history[key]) * 90) // 100 : ])\n",
    "        plt.plot((epochs[start_x_from], epochs[-1]), (late_avg, late_avg),\n",
    "                 color=\"#74E600\", label='Mean {:.3f}'.format(late_avg))\n",
    "        if valkey in history:\n",
    "            plt.plot(epochs[start_x_from:], history[valkey][start_x_from:], label='Validation ' + key,\n",
    "                    marker='+', color='#DF004F', linestyle='')\n",
    "            \n",
    "            late_avg = np.mean(history[valkey][(len(history[valkey]) * 90) // 100 : ])\n",
    "            plt.plot((epochs[start_x_from], epochs[-1]), (late_avg, late_avg),\n",
    "                     color=\"#FF6700\", label='Mean {:.3f}'.format(late_avg))\n",
    "        plt.legend()\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_same_length(*args):\n",
    "    \"\"\"Returns True if all arguments have equal len(.)\"\"\"\n",
    "    return all(len(a) == len(args[0]) for a in args)\n",
    "\n",
    "def shuffle_together(*numpy_arrays):\n",
    "    \"\"\"Shuffles numpy arrays in unison, returns a tuple.\n",
    "    \n",
    "    (applies the same random permutation to all of them,\n",
    "    so they have to be the same length on axis=0)\"\"\"\n",
    "    \n",
    "    assert all_same_length(*numpy_arrays)\n",
    "    permut = np.random.permutation(len(numpy_arrays[0]))\n",
    "    return tuple(a[permut] for a in numpy_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the dataset & apply transformations\n",
    "\n",
    "(x_train_orig, y_train_orig), (x_test, y_test) = keras.datasets\\\n",
    "                                        .fashion_mnist.load_data()\n",
    "\n",
    "y_train_orig = keras.utils.to_categorical(y_train_orig)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "x_train_orig = x_train_orig.reshape(*x_train_orig.shape, 1).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(*x_test.shape, 1).astype('float32') / 255.0\n",
    "\n",
    "SPLIT = 10000\n",
    "x_valid = x_train_orig[:SPLIT]\n",
    "y_valid = y_train_orig[:SPLIT]\n",
    "\n",
    "x_train = x_train_orig[SPLIT:]\n",
    "y_train = y_train_orig[SPLIT:]\n",
    "\n",
    "INPUT_SHAPE = x_train[0].shape\n",
    "VERBOSE=False\n",
    "FIT_SETS = dict(x=x_train, y=y_train, validation_data=(x_valid, y_valid))\n",
    "STATS_STR = ('Last epoch {epoch:}\\t|'     '  Training  |'          '  Validation  |{nl:}'  \n",
    "             'Loss \\t\\t|'          '  {loss[0]: 8.4f}  |''  {val_loss[0]: 10.4f}  |{nl:}'\n",
    "             'Acc  \\t\\t|'           '  {acc[0]: 8.4f}  |' '  {val_acc[0]: 10.4f}  |')\n",
    "BS = 500\n",
    "EPOCHS=50\n",
    "GEN_KWARGS = dict(samplewise_center=True, samplewise_std_normalization=True)\n",
    "\n",
    "train_gen = ImageDataGenerator(**GEN_KWARGS, horizontal_flip=True)\n",
    "train_data_flow = train_gen.flow(x_train_orig, y_train_orig, batch_size=BS)\n",
    "\n",
    "test_gen = ImageDataGenerator(**GEN_KWARGS)\n",
    "test_data_flow = test_gen.flow(x_test, y_test, batch_size=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 8)         80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 8)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                100416    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 101,146\n",
      "Trainable params: 101,146\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-622786bd00ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mhist_simple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_simple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mFIT_SETS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVERBOSE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# baseline (validation accuracy about 0.90)\n",
    "# its siblings were reaching accuracy up to 0.92 (and blazingly fast to train)\n",
    "model_simple = build_model(\n",
    "    kl.InputLayer(INPUT_SHAPE),\n",
    "    \n",
    "    kl.Conv2D(filters=8, kernel_size=3, activation='relu', padding='same'),\n",
    "    kl.MaxPool2D(pool_size=2, strides=2),\n",
    "    \n",
    "    kl.Flatten(),\n",
    "    \n",
    "    kl.Dense(units=64, activation='relu'),\n",
    "    kl.Dense(units=10, activation='softmax'),\n",
    "    \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "hist_simple = model_simple.fit(**FIT_SETS, epochs=EPOCHS, batch_size=BS, verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(hist_simple, start_x_from=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet-5 (accuracy up to 0.90)\n",
    "model_lenet5 = build_model(\n",
    "    kl.InputLayer(INPUT_SHAPE),\n",
    "    \n",
    "    kl.Conv2D(filters=6, kernel_size=5, activation='relu', padding='same'),\n",
    "    kl.MaxPool2D(pool_size=2, strides=2),\n",
    "    \n",
    "    kl.Conv2D(filters=16, kernel_size=5, activation='relu', padding='valid'),\n",
    "    kl.MaxPool2D(pool_size=2, strides=2),\n",
    "    \n",
    "    kl.Flatten(),\n",
    "    \n",
    "    kl.Dense(units=120, activation='relu'),\n",
    "    kl.Dense(units=84, activation='relu'),\n",
    "    kl.Dense(units=10, activation='softmax'),\n",
    "    \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "hist_lenet5 = model_lenet5.fit(**FIT_SETS, epochs=EPOCHS, batch_size=BS, verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(hist_lenet5, start_x_from=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quite poor just about 0.915\n",
    "# LeNet\n",
    "# https://www.pyimagesearch.com/2016/08/01/lenet-convolutional-neural-network-in-python/\n",
    "model_lenet = build_model(\n",
    "    kl.InputLayer(INPUT_SHAPE),\n",
    "\n",
    "    kl.Conv2D(filters=20, kernel_size=5, padding='same', activation='relu'),\n",
    "    kl.MaxPool2D(pool_size=2, strides=2),\n",
    "\n",
    "\n",
    "    kl.Conv2D(filters=50, kernel_size=5, padding='same', activation='relu'),\n",
    "    kl.MaxPool2D(pool_size=2, strides=2),\n",
    "\n",
    "    kl.Flatten(),\n",
    "\n",
    "    kl.Dense(units=500, activation='relu'),\n",
    "    kl.Dense(units=10, activation='softmax'),\n",
    "    \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "hist_lenet = model_lenet.fit(**FIT_SETS, epochs=EPOCHS, batch_size=BS, verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(hist_lenet, start_x_from=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quite poor just about 0.915\n",
    "# LeNet\n",
    "# https://www.pyimagesearch.com/2016/08/01/lenet-convolutional-neural-network-in-python/\n",
    "model_lenet_reg = build_model(\n",
    "    kl.InputLayer(INPUT_SHAPE),\n",
    "\n",
    "    kl.Conv2D(filters=20, kernel_size=5, padding='same', activation='relu'),\n",
    "    kl.Dropout(0.3),\n",
    "    kl.MaxPool2D(pool_size=2, strides=2),\n",
    "\n",
    "\n",
    "    kl.Conv2D(filters=50, kernel_size=5, padding='same', activation='relu'),\n",
    "    kl.Dropout(0.3),\n",
    "    kl.MaxPool2D(pool_size=2, strides=2),\n",
    "\n",
    "    kl.Flatten(),\n",
    "\n",
    "    kl.Dropout(0.3),\n",
    "    kl.Dense(units=500, activation='relu'),\n",
    "    kl.Dense(units=10, activation='softmax'),\n",
    "    \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "hist_lenet_reg = model_lenet_reg.fit(**FIT_SETS, epochs=EPOCHS, batch_size=BS, verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(hist_lenet_reg, start_x_from=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quite complex, but starving model (0.91)\n",
    "model_starving = build_model(\n",
    "    kl.InputLayer(INPUT_SHAPE),\n",
    "    \n",
    "    kl.Conv2D(filters=20, kernel_size=5, padding='same', activation='relu'),\n",
    "    kl.Conv2D(filters=30, kernel_size=5, padding='same', activation='relu'),\n",
    "    kl.MaxPool2D(pool_size=2, strides=2),\n",
    "\n",
    "    kl.Conv2D(filters=20, kernel_size=5, padding='same', activation='relu'),\n",
    "    kl.Conv2D(filters=30, kernel_size=3, activation='relu'),\n",
    "    kl.MaxPool2D(pool_size=2, strides=2),\n",
    "\n",
    "    kl.Conv2D(filters=20, kernel_size=3, padding='same', activation='relu'),\n",
    "    kl.Conv2D(filters=20, kernel_size=3, activation='relu'),\n",
    "    kl.MaxPool2D(pool_size=2, strides=2),\n",
    "\n",
    "    kl.Flatten(),\n",
    "\n",
    "    kl.Dense(units=80, activation='relu'),\n",
    "    kl.Dense(units=80, activation='relu'),\n",
    "    kl.Dense(units=10, activation='softmax'),\n",
    "    \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "hist_starving = model_starving.fit(**FIT_SETS, epochs=EPOCHS, batch_size=BS, verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(hist_starving, start_x_from=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the final model\n",
    "model_final = build_model(\n",
    "    kl.InputLayer(INPUT_SHAPE),\n",
    "\n",
    "    kl.Conv2D(filters=20, kernel_size=3, activation='relu', padding='same' ),\n",
    "    kl.Conv2D(filters=40, kernel_size=3, activation='relu', padding='same' ),\n",
    "    kl.Dropout(0.2),\n",
    "    kl.MaxPool2D(),\n",
    "\n",
    "    kl.Conv2D(filters=40, kernel_size=3, activation='relu', padding='same' ),\n",
    "    kl.Dropout(0.2),\n",
    "    kl.MaxPool2D(strides=1),\n",
    "\n",
    "    kl.Conv2D(filters=40, kernel_size=3, activation='relu', padding='same' ),\n",
    "    kl.Dropout(0.2),\n",
    "    kl.MaxPool2D(strides=1),\n",
    "\n",
    "    kl.Flatten(),\n",
    "\n",
    "    kl.Dropout(0.35),\n",
    "    kl.Dense(units=130, activation='relu'),\n",
    "    kl.Dropout(0.5),\n",
    "    kl.Dense(units=105, activation='relu'),\n",
    "    kl.Dropout(0.4),\n",
    "    kl.Dense(units=10, activation='softmax'),\n",
    "\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "epoch = 0\n",
    "history = {k:[] for k in ['val_loss', 'val_acc', 'loss', 'acc']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while epoch < 1500:\n",
    "    last_history = model_final.fit_generator(generator=train_data_flow,\n",
    "                                    validation_data=test_data_flow,\n",
    "                                    steps_per_epoch=len(x_train_orig) // BS,\n",
    "                                    epochs=epoch+1, initial_epoch=epoch,\n",
    "                                    verbose=0)\n",
    "    \n",
    "    for k, v in last_history.history.items():\n",
    "        history[k].append(v[0])\n",
    "        \n",
    "    print(STATS_STR.format(nl=' '*100, epoch=epoch, **last_history.history), end='\\r')\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, start_x_from=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the final model\n",
    "model_final2 = build_model(\n",
    "    kl.InputLayer(INPUT_SHAPE),\n",
    "\n",
    "    kl.Conv2D(filters=20, kernel_size=3, activation='relu', padding='same' ),\n",
    "    kl.Conv2D(filters=40, kernel_size=3, activation='relu', padding='same' ),\n",
    "    kl.Dropout(0.2),\n",
    "    kl.MaxPool2D(),\n",
    "\n",
    "    kl.Conv2D(filters=40, kernel_size=3, activation='relu', padding='same' ),\n",
    "    kl.Dropout(0.2),\n",
    "    kl.MaxPool2D(strides=1),\n",
    "\n",
    "    kl.Conv2D(filters=40, kernel_size=3, activation='relu', padding='same' ),\n",
    "    kl.Dropout(0.2),\n",
    "    kl.MaxPool2D(strides=1),\n",
    "\n",
    "    kl.Flatten(),\n",
    "\n",
    "    kl.Dropout(0.35),\n",
    "    kl.Dense(units=100, activation='relu'),\n",
    "    kl.Dropout(0.5),\n",
    "    kl.Dense(units=100, activation='relu'),\n",
    "    kl.Dropout(0.4),\n",
    "    kl.Dense(units=10, activation='softmax'),\n",
    "\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "epoch2 = 0\n",
    "history2 = {k:[] for k in ['val_loss', 'val_acc', 'loss', 'acc']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while epoch2 < 1500:\n",
    "    last_history = model_final2.fit_generator(generator=train_data_flow,\n",
    "                                    validation_data=test_data_flow,\n",
    "                                    steps_per_epoch=len(x_train_orig) // BS,\n",
    "                                    epochs=epoch2+1, initial_epoch=epoch2,\n",
    "                                    verbose=0)\n",
    "    \n",
    "    for k, v in last_history.history.items():\n",
    "        history2[k].append(v[0])\n",
    "        \n",
    "    print(STATS_STR.format(nl=' '*100, epoch=epoch2, **last_history.history), end='\\r')\n",
    "    epoch2 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history2, start_x_from=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
