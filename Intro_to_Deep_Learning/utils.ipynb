{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequently used scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found out that trying to use GPU can be very tricky. I though my TensorFlow was not utilizing my GPU as setting up dedicated and integrated  graphics cards on a laptop can get rather obscure in Linux, I gained remote access to my brother's computer (with 8GB GeForce GTX 1070) running Windows and thought I would set everything up there (that was pain on its own as the docs for Windows are outdated), I successfully prepared everything and in the mean time found out that my computer indeed was not using the GPU. With a lot of enthusiasm, I opened up some of the previous exercises and timed retraining them just to find out that the performance (on the awesome GPU) was more than 4 times slower... It was running on the GPU there, but it had only about 10% utilization, after some research I found out the builds for Windows are poor and trying to optimize it and build from sources was the last thing I wanted to do in Windows... So I came back to my machine, fixed my issues, made sure TensorFlow was capable of running on my GPU (GeForce GTX 1050 mobile, 2GB), timed it again and once more to find out that is was slooooow - about 2 times slower than just on my CPU (i5-7300HQ CPU @ 2.50GHz with 8GB RAM). I purged the whole TensorFlow, carefully went through its installation from source documentation, configured and built with optimization for my hardware. Now the CPU's performance has increased by 15% and I am getting almost exactly 2 times slower performance on the GPU compared to the CPU... The GPU utilization is 15% - 20%, according to forums it just might be that the models/inputs are just too small to have high utilization - other than that I have no idea why it is so slow..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for testing whether CPU or GPU is faster on a problem\n",
    "NO_GPU = False\n",
    "\n",
    "if NO_GPU:\n",
    "    import os\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_simple_model(neurons, activations, input_shape, verbose=True,\n",
    "               optimizer='sgd', loss='categorical_crossentropy', metrics=['acc']):\n",
    "    model = keras.models.Sequential()\n",
    "    zzz = zip(neurons, activations)\n",
    "    \n",
    "    n, a = next(zzz)\n",
    "    model.add(keras.layers.Dense(n, input_shape=input_shape, activation=a))\n",
    "    for n, a in zzz:\n",
    "        model.add(keras.layers.Dense(n, activation=a))\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"Model summary:\")\n",
    "        model.summary()\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(*layers, verbose=False,\n",
    "               optimizer='adam', loss='categorical_crossentropy', metrics=['acc'],\n",
    "               compile_kwargs={}):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    for layer in layers:\n",
    "        model.add(layer)\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"Model summary:\")\n",
    "        model.summary()\n",
    "    \n",
    "    for kw in ('optimizer', 'loss', 'metrics'):\n",
    "        if not kw in compile_kwargs:\n",
    "            compile_kwargs[kw] = locals()[kw]\n",
    "    model.compile(**compile_kwargs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_history(history, figsize=(15,4), title='', columns=2, start_x_from=0):\n",
    "    \"\"\"Graphs a history for each key (combines validation and training keys into one plot).\n",
    "    \n",
    "    start_x_from=N skips the first N entries.\n",
    "    \n",
    "    History can be a whole training history class or just a dict.\"\"\"\n",
    "    \n",
    "    if hasattr(history, 'history'): # full history given\n",
    "        history = history.history   # only that history is enough\n",
    "        \n",
    "    assert hasattr(history, 'keys')\n",
    "    keys = [key for key in history.keys() if not key.startswith(\"val_\")]\n",
    "    assert keys # there is one at least\n",
    "    epochs = list(range(1,len(history[keys[0]])+1)) # all should have the same size list\n",
    "    \n",
    "    rows = np.ceil(len(keys)/columns).astype('int')\n",
    "    \n",
    "    fig=plt.figure(figsize=figsize)\n",
    "    f = plt.title(title)\n",
    "    f.axes.get_xaxis().set_visible(False)\n",
    "    f.axes.get_yaxis().set_visible(False)\n",
    "    \n",
    "    i = 1\n",
    "    for key in sorted(keys):\n",
    "        valkey = \"val_\" + key\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        i += 1\n",
    "        plt.plot(epochs[start_x_from:], history[key][start_x_from:], label=\"Training \" + key,\n",
    "                 marker='.', color='#00A287', linestyle='')\n",
    "        \n",
    "        late_avg = np.mean(history[key][(len(history[key]) * 90) // 100 : ])\n",
    "        plt.plot((epochs[start_x_from], epochs[-1]), (late_avg, late_avg),\n",
    "                 color=\"#74E600\", label='Mean {:.3f}'.format(late_avg))\n",
    "        if valkey in history:\n",
    "            plt.plot(epochs[start_x_from:], history[valkey][start_x_from:], label='Validation ' + key,\n",
    "                    marker='+', color='#DF004F', linestyle='')\n",
    "            \n",
    "            late_avg = np.mean(history[valkey][(len(history[valkey]) * 90) // 100 : ])\n",
    "            plt.plot((epochs[start_x_from], epochs[-1]), (late_avg, late_avg),\n",
    "                     color=\"#FF6700\", label='Mean {:.3f}'.format(late_avg))\n",
    "        plt.legend()\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_imgs(images, columns=9, figsize=(15,7), title=''):\n",
    "    \"\"\"Displays images in a grid\"\"\"\n",
    "    \n",
    "    fig=plt.figure(figsize=figsize)\n",
    "    f = plt.title(title)\n",
    "    \n",
    "    f.axes.get_xaxis().set_visible(False)\n",
    "    f.axes.get_yaxis().set_visible(False)\n",
    "    \n",
    "    rows = np.ceil(len(images)/columns).astype('int')\n",
    "    \n",
    "    for i in range(1, len(images)+1):\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        f = plt.imshow(images[i-1], cmap=plt.cm.binary)\n",
    "        f.axes.get_xaxis().set_visible(False)\n",
    "        f.axes.get_yaxis().set_visible(False)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_vals(img):\n",
    "    \"\"\"Shows values in a 2D normalized grayscale image.\"\"\"\n",
    "    for r in img:\n",
    "        for c in r:\n",
    "            if c:\n",
    "                print('{: 2.0f}'.format(c*100), end='', sep='')\n",
    "            else:\n",
    "                print('  ', end='', sep='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_same_length(*args):\n",
    "    \"\"\"Returns True if all arguments have equal len(.)\"\"\"\n",
    "    return all(len(a) == len(args[0]) for a in args)\n",
    "\n",
    "def shuffle_together(*numpy_arrays):\n",
    "    \"\"\"Shuffles numpy arrays in unison, returns a tuple.\n",
    "    \n",
    "    (applies the same random permutation to all of them,\n",
    "    so they have to be the same length on axis=0)\"\"\"\n",
    "    \n",
    "    assert all_same_length(*numpy_arrays)\n",
    "    permut = np.random.permutation(len(numpy_arrays[0]))\n",
    "    return tuple(a[permut] for a in numpy_arrays)\n",
    "\n",
    "def normalize(shift, scale, *args):\n",
    "    \"\"\"For each arg returns: (arg + shift) * scale\"\"\"\n",
    "    return tuple((arg + shift) * scale for arg in args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inner_slice_from_k_split(k, ith_slice, numpy_array):\n",
    "    \"\"\"returns the validation part of the data\"\"\"\n",
    "    split_length = len(numpy_array) // k \n",
    "    return numpy_array[ith_slice * split_length : (ith_slice + 1) * split_length]\n",
    "    \n",
    "def get_outer_slice_from_k_split(k, ith_slice, numpy_array):\n",
    "    \"\"\"returns the training part of the data\"\"\"\n",
    "    split_length = len(numpy_array) // k \n",
    "    return np.concatenate((\n",
    "        numpy_array[:ith_slice * split_length],\n",
    "        numpy_array[(ith_slice + 1) * split_length:]),\n",
    "        axis=0)\n",
    "\n",
    "def get_k_fold_sets(k, ith_set, x_data, y_targets):\n",
    "    \"\"\"returns the i-th training/validation set in k-fold,\n",
    "    returns as: (x_train, y_train), (x_valid, y_valid)\"\"\"\n",
    "    \n",
    "    x_train = get_outer_slice_from_k_split(k, ith_set, x_data)\n",
    "    y_train = get_outer_slice_from_k_split(k, ith_set, y_targets)\n",
    "    \n",
    "    x_valid = get_inner_slice_from_k_split(k, ith_set, x_data)\n",
    "    y_valid = get_inner_slice_from_k_split(k, ith_set, y_targets)\n",
    "    \n",
    "    return (x_train, y_train), (x_valid, y_valid)\n",
    "\n",
    "def kfoldize(K, train_data, train_targets,\n",
    "             epochs, batch_size,\n",
    "             model_factory,\n",
    "             verbose_level=1):\n",
    "    \"\"\"Runs K-fold validation on the supplied dataset.\n",
    "    Returns the training histories for all K trainings as a list.\n",
    "    \n",
    "    model_factory must be callable and return the desired model (fresh copy)\n",
    "    \n",
    "    Any non-false verbose_level informs upon begining new training,\n",
    "    verbose_level >= 10 sets trainings to be verbose.\"\"\"\n",
    "    \n",
    "    histories = []\n",
    "    for i in range(K):\n",
    "    \n",
    "        if verbose_level:\n",
    "            print('fold', i+1, 'out of', K)\n",
    "    \n",
    "        kfold_training, kfold_validation = get_k_fold_sets(K, i, train_data, train_targets)\n",
    "        model = model_factory()\n",
    "        hist = model.fit(*kfold_training, validation_data=kfold_validation,\n",
    "                        epochs=epochs, batch_size=batch_size, verbose=(verbose_level >= 10))\n",
    "        histories.append(hist.history)\n",
    "        \n",
    "    return histories\n",
    "\n",
    "def average_histories(histories, for_keys=None):\n",
    "    \"\"\"Averages the results per key,\n",
    "    if for_keys==None, all keys are used,\n",
    "    otherwise a key in for_keys must be\n",
    "    containded in each history\"\"\"\n",
    "    \n",
    "    if for_keys == None:\n",
    "        for_keys = histories[0].keys()\n",
    "        \n",
    "    average_history = {}\n",
    "    for key in for_keys:\n",
    "        average_history[key] = [\n",
    "            np.mean([hist[key][epoch] for hist in histories]) for epoch in range(len(histories[0][key]))]\n",
    "        \n",
    "    return average_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
