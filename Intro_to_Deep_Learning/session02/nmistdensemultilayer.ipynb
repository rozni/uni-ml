{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code here is unchanged except for the number of epochs for learning (not to overfit...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/martin/Uni/Intro_to_Deep_Learning/session02\n",
      "2.1.2\n",
      "1.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAADMBJREFUeJzt3X+IXfWZx/H3s7Otf2jxB45jtGpq\nEbMirF0GXXRds4pil2KsUGn+qFFKU/AHFopsELH+4UJc1nYVlkK6CUZobQutPxDRRll/wVIcJTR2\n3d2KjG02IZlBRfuPxfjsH3PTnercM+P9de7keb8g3HvPc+75PhzymXPuPffeb2Qmkur5s7YbkNQO\nwy8VZfilogy/VJThl4oy/FJRhl8qyvBLRRl+qag/H+VgJ554Yq5du3aUQ0qlzM7OMj8/HytZt6/w\nR8SVwH3ABPBvmbm1af21a9cyMzPTz5CSGkxPT6943Z5P+yNiAvhX4IvAOcDGiDin1+1JGq1+XvOf\nD7yemW9k5h+AHwMbBtOWpGHrJ/ynAr9b9HhvZ9mfiIjNETETETNzc3N9DCdpkPoJ/1JvKnzs+8GZ\nuS0zpzNzenJyso/hJA1SP+HfC5y26PFngX39tSNpVPoJ/0vAWRHxuYj4NPBV4LHBtCVp2Hq+1JeZ\nH0TEzcBTLFzq25GZvx5YZ5KGqq/r/Jn5BPDEgHqRNEJ+vFcqyvBLRRl+qSjDLxVl+KWiDL9UlOGX\nijL8UlGGXyrK8EtFGX6pKMMvFWX4paIMv1SU4ZeKMvxSUYZfKsrwS0UZfqkowy8VZfilogy/VJTh\nl4oy/FJRhl8qyvBLRRl+qSjDLxVl+KWi+pqlNyJmgfeAQ8AHmTk9iKYkgLvvvruxfueddzbWM7Nr\n7dlnn2187iWXXNJYPxL0Ff6Ov8vM+QFsR9IIedovFdVv+BP4RUS8HBGbB9GQpNHo97T/oszcFxEn\nAbsi4r8y8/nFK3T+KGwGOP300/scTtKg9HXkz8x9nduDwMPA+Uussy0zpzNzenJysp/hJA1Qz+GP\niKMj4jOH7wNXAK8OqjFJw9XPaf8U8HBEHN7OjzLzyYF0JWnoeg5/Zr4B/OUAe1ExDzzwQGN969at\njfWJiYnG+qFDh7rWOget0rzUJxVl+KWiDL9UlOGXijL8UlGGXypqEN/qk3ry5ptvNtbff//9EXVS\nk0d+qSjDLxVl+KWiDL9UlOGXijL8UlGGXyrK6/waqqeffrpr7f777+9r2+vWrWusP/74411rU1NT\nfY19JPDILxVl+KWiDL9UlOGXijL8UlGGXyrK8EtFeZ1ffXnxxRcb69dff33X2rvvvtvX2Lfddltj\n/Ywzzuhr+0c6j/xSUYZfKsrwS0UZfqkowy8VZfilogy/VNSy1/kjYgfwJeBgZp7bWXYC8BNgLTAL\nXJuZbw+vTY2rnTt3Ntb37dvX87bXr1/fWL/uuut63rZWduR/ALjyI8u2AM9k5lnAM53HklaRZcOf\nmc8Db31k8Qbg8J/8ncDVA+5L0pD1+pp/KjP3A3RuTxpcS5JGYehv+EXE5oiYiYiZubm5YQ8naYV6\nDf+BiFgD0Lk92G3FzNyWmdOZOT05OdnjcJIGrdfwPwZs6tzfBDw6mHYkjcqy4Y+Ih4D/AM6OiL0R\n8XVgK3B5RPwGuLzzWNIqsux1/szc2KV02YB70Rian59vrG/fvr2xPjEx0bV23HHHNT73jjvuaKyr\nP37CTyrK8EtFGX6pKMMvFWX4paIMv1SUP91d3OzsbGP9mmuuGdrYt9xyS2P90ksvHdrY8sgvlWX4\npaIMv1SU4ZeKMvxSUYZfKsrwS0V5nb+4J598srG+Z8+evrZ/2WXdv/l966239rVt9ccjv1SU4ZeK\nMvxSUYZfKsrwS0UZfqkowy8V5XX+I9wjjzzSWN+ypb8Jli+++OLGetMU3scee2xfY6s/Hvmlogy/\nVJThl4oy/FJRhl8qyvBLRRl+qahlr/NHxA7gS8DBzDy3s+wu4BvAXGe12zPziWE1qWZNv70/zN/d\nBzjzzDMb61NTU0MdX71byZH/AeDKJZZ/LzPP6/wz+NIqs2z4M/N54K0R9CJphPp5zX9zRPwqInZE\nxPED60jSSPQa/u8DnwfOA/YD93ZbMSI2R8RMRMzMzc11W03SiPUU/sw8kJmHMvND4AfA+Q3rbsvM\n6cycnpyc7LVPSQPWU/gjYs2ih18GXh1MO5JGZSWX+h4C1gMnRsRe4DvA+og4D0hgFvjmEHuUNATL\nhj8zNy6xePsQelGP7rnnnq61iYmJoY7d7+8BqD1+wk8qyvBLRRl+qSjDLxVl+KWiDL9UlD/dvQrs\n3r27sf7UU08NbeyrrrqqsX722WcPbWwNl0d+qSjDLxVl+KWiDL9UlOGXijL8UlGGXyrK6/yrwBVX\nXNFYf/vtt3ve9gUXXNBYb5piW6ubR36pKMMvFWX4paIMv1SU4ZeKMvxSUYZfKsrr/KvA/Px8Y72f\nn+e+6aabGuvHHHNMz9vWePPILxVl+KWiDL9UlOGXijL8UlGGXyrK8EtFLXudPyJOAx4ETgY+BLZl\n5n0RcQLwE2AtMAtcm5m9f7G8sBtuuKGxnpmN9UOHDvU89oUXXtjzc7W6reTI/wHw7cz8C+CvgZsi\n4hxgC/BMZp4FPNN5LGmVWDb8mbk/M1/p3H8PeA04FdgAHP6Zl53A1cNqUtLgfaLX/BGxFvgC8Etg\nKjP3w8IfCOCkQTcnaXhWHP6IOAb4GfCtzHz3Ezxvc0TMRMTM3NxcLz1KGoIVhT8iPsVC8H+YmT/v\nLD4QEWs69TXAwaWem5nbMnM6M6cnJycH0bOkAVg2/BERwHbgtcz87qLSY8Cmzv1NwKODb0/SsKzk\nK70XAV8D9kTE4bmibwe2Aj+NiK8DvwW+MpwWV7/lptjetWtXY33h7293Rx11VNfajTfe2Pjcqamp\nxrqOXMuGPzNfBLr977tssO1IGhU/4ScVZfilogy/VJThl4oy/FJRhl8qyp/uHoF33nmnsX7gwIG+\ntn/KKad0rd177719bVtHLo/8UlGGXyrK8EtFGX6pKMMvFWX4paIMv1SU4ZeKMvxSUYZfKsrwS0UZ\nfqkowy8VZfilogy/VJTf5x+BdevWNdaXmyb7hRdeGGQ7EuCRXyrL8EtFGX6pKMMvFWX4paIMv1SU\n4ZeKWvY6f0ScBjwInAx8CGzLzPsi4i7gG8BcZ9XbM/OJYTW6mp188smN9eeee25EnUj/byUf8vkA\n+HZmvhIRnwFejohdndr3MvOfh9eepGFZNvyZuR/Y37n/XkS8Bpw67MYkDdcnes0fEWuBLwC/7Cy6\nOSJ+FRE7IuL4Ls/ZHBEzETEzNze31CqSWrDi8EfEMcDPgG9l5rvA94HPA+excGaw5KRwmbktM6cz\nc3pycnIALUsahBWFPyI+xULwf5iZPwfIzAOZeSgzPwR+AJw/vDYlDdqy4Y+IALYDr2XmdxctX7No\ntS8Drw6+PUnDspJ3+y8CvgbsiYjdnWW3Axsj4jwggVngm0PpUNJQrOTd/heBWKLkNX1pFfMTflJR\nhl8qyvBLRRl+qSjDLxVl+KWiDL9UlOGXijL8UlGGXyrK8EtFGX6pKMMvFWX4paIiM0c3WMQc8Oai\nRScC8yNr4JMZ197GtS+wt14NsrczMnNFv5c30vB/bPCImcycbq2BBuPa27j2BfbWq7Z687RfKsrw\nS0W1Hf5tLY/fZFx7G9e+wN561Upvrb7ml9Seto/8klrSSvgj4sqI+O+IeD0itrTRQzcRMRsReyJi\nd0TMtNzLjog4GBGvLlp2QkTsiojfdG6XnCatpd7uioj/7ey73RHx9y31dlpE/HtEvBYRv46IWzvL\nW913DX21st9GftofERPA/wCXA3uBl4CNmfmfI22ki4iYBaYzs/VrwhHxt8DvgQcz89zOsn8C3srM\nrZ0/nMdn5j+MSW93Ab9ve+bmzoQyaxbPLA1cDVxPi/uuoa9raWG/tXHkPx94PTPfyMw/AD8GNrTQ\nx9jLzOeBtz6yeAOws3N/Jwv/eUauS29jITP3Z+YrnfvvAYdnlm513zX01Yo2wn8q8LtFj/cyXlN+\nJ/CLiHg5Ija33cwSpjrTph+ePv2klvv5qGVnbh6lj8wsPTb7rpcZrwetjfAvNfvPOF1yuCgz/wr4\nInBT5/RWK7OimZtHZYmZpcdCrzNeD1ob4d8LnLbo8WeBfS30saTM3Ne5PQg8zPjNPnzg8CSpnduD\nLffzR+M0c/NSM0szBvtunGa8biP8LwFnRcTnIuLTwFeBx1ro42Mi4ujOGzFExNHAFYzf7MOPAZs6\n9zcBj7bYy58Yl5mbu80sTcv7btxmvG7lQz6dSxn/AkwAOzLzH0fexBIi4kwWjvawMInpj9rsLSIe\nAtaz8K2vA8B3gEeAnwKnA78FvpKZI3/jrUtv61k4df3jzM2HX2OPuLe/AV4A9gAfdhbfzsLr69b2\nXUNfG2lhv/kJP6koP+EnFWX4paIMv1SU4ZeKMvxSUYZfKsrwS0UZfqmo/wOwvY+JjyoCowAAAABJ\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe17fe72da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe17c9e7908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing import image\n",
    "from scipy import misc\n",
    "\n",
    "#mimg = load_img('my_numbers/0s/0-02.bmp', target_size=(28,28), grayscale=True)\n",
    "#mimg = image.img_to_array(mimg)\n",
    "#mimg = 1-mimg.reshape((28, 28)).astype('float32') / 255.0\n",
    "#print(type(mimg), mimg.shape)\n",
    "#plt.imshow(mimg, cmap=plt.cm.binary)\n",
    "#plt.show()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "#print(\"shapes are:\", x_train.shape, y_train.shape)\n",
    "\n",
    "img1 = x_train[314].reshape(-1, 28*28).astype('float32') / 255.0\n",
    "img1label = y_train[314]\n",
    "\n",
    "\"\"\"\n",
    "img=x_train[3]\n",
    "for r in img:\n",
    "    for c in r:\n",
    "        if c:\n",
    "            print('()', end='', sep='')\n",
    "        else:\n",
    "            print('  ', end='', sep='')\n",
    "    print()\n",
    "print(y_train[3])\"\"\"\n",
    "\n",
    "img = x_train[3]\n",
    "plt.imshow(img, cmap=plt.cm.binary)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(img1.reshape(28,28), cmap=plt.cm.gray)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "x_train = x_train.reshape(-1, 28 * 28).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 28 * 28).astype('float32') / 255.0\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                260       \n",
      "=================================================================\n",
      "Total params: 88,285\n",
      "Trainable params: 88,285\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(100, input_shape=(28 * 28,), activation='relu'))\n",
    "model.add(keras.layers.Dense(50, activation='relu'))\n",
    "model.add(keras.layers.Dense(50, activation='relu'))\n",
    "model.add(keras.layers.Dense(25, activation='relu'))\n",
    "model.add(keras.layers.Dense(25, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/60\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 1.4987 - acc: 0.5015 - val_loss: 0.5783 - val_acc: 0.8126\n",
      "Epoch 2/60\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.4396 - acc: 0.8682 - val_loss: 0.3392 - val_acc: 0.8974\n",
      "Epoch 3/60\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.3009 - acc: 0.9121 - val_loss: 0.2606 - val_acc: 0.9244\n",
      "Epoch 4/60\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.2407 - acc: 0.9296 - val_loss: 0.2360 - val_acc: 0.9305\n",
      "Epoch 5/60\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.2033 - acc: 0.9405 - val_loss: 0.1926 - val_acc: 0.9432\n",
      "Epoch 6/60\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.1771 - acc: 0.9479 - val_loss: 0.1745 - val_acc: 0.9489\n",
      "Epoch 7/60\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.1566 - acc: 0.9543 - val_loss: 0.1662 - val_acc: 0.9519\n",
      "Epoch 8/60\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.1411 - acc: 0.9589 - val_loss: 0.1570 - val_acc: 0.9520\n",
      "Epoch 9/60\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.1292 - acc: 0.9626 - val_loss: 0.1369 - val_acc: 0.9598\n",
      "Epoch 10/60\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.1187 - acc: 0.9654 - val_loss: 0.1409 - val_acc: 0.9593\n",
      "Epoch 11/60\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.1097 - acc: 0.9682 - val_loss: 0.1436 - val_acc: 0.9571\n",
      "Epoch 12/60\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.1012 - acc: 0.9707 - val_loss: 0.1252 - val_acc: 0.9637\n",
      "Epoch 13/60\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0951 - acc: 0.9726 - val_loss: 0.1199 - val_acc: 0.9665\n",
      "Epoch 14/60\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0879 - acc: 0.9743 - val_loss: 0.1210 - val_acc: 0.9649\n",
      "Epoch 15/60\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0830 - acc: 0.9760 - val_loss: 0.1208 - val_acc: 0.9644\n",
      "Epoch 16/60\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0775 - acc: 0.9779 - val_loss: 0.1333 - val_acc: 0.9610\n",
      "Epoch 17/60\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0730 - acc: 0.9788 - val_loss: 0.1103 - val_acc: 0.9694\n",
      "Epoch 18/60\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0686 - acc: 0.9800 - val_loss: 0.1275 - val_acc: 0.9626\n",
      "Epoch 19/60\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0646 - acc: 0.9813 - val_loss: 0.1249 - val_acc: 0.9643\n",
      "Epoch 20/60\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0601 - acc: 0.9825 - val_loss: 0.1109 - val_acc: 0.9692\n",
      "Epoch 21/60\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0572 - acc: 0.9834 - val_loss: 0.1046 - val_acc: 0.9727\n",
      "Epoch 22/60\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0536 - acc: 0.9844 - val_loss: 0.1134 - val_acc: 0.9689\n",
      "Epoch 23/60\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0507 - acc: 0.9855 - val_loss: 0.0995 - val_acc: 0.9736\n",
      "Epoch 24/60\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0466 - acc: 0.9866 - val_loss: 0.1083 - val_acc: 0.9708\n",
      "Epoch 25/60\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0442 - acc: 0.9873 - val_loss: 0.1043 - val_acc: 0.9723\n",
      "Epoch 26/60\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0421 - acc: 0.9876 - val_loss: 0.1162 - val_acc: 0.9696\n",
      "Epoch 27/60\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0387 - acc: 0.9892 - val_loss: 0.1033 - val_acc: 0.9734\n",
      "Epoch 28/60\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0360 - acc: 0.9901 - val_loss: 0.1151 - val_acc: 0.9691\n",
      "Epoch 29/60\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0347 - acc: 0.9903 - val_loss: 0.1098 - val_acc: 0.9709\n",
      "Epoch 30/60\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0324 - acc: 0.9908 - val_loss: 0.1117 - val_acc: 0.9712\n",
      "Epoch 31/60\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0300 - acc: 0.9915 - val_loss: 0.1053 - val_acc: 0.9732\n",
      "Epoch 32/60\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0274 - acc: 0.9925 - val_loss: 0.1204 - val_acc: 0.9694\n",
      "Epoch 33/60\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0249 - acc: 0.9934 - val_loss: 0.1084 - val_acc: 0.9731\n",
      "Epoch 34/60\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0237 - acc: 0.9938 - val_loss: 0.1134 - val_acc: 0.9711\n",
      "Epoch 35/60\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0215 - acc: 0.9947 - val_loss: 0.1130 - val_acc: 0.9721\n",
      "Epoch 36/60\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0208 - acc: 0.9945 - val_loss: 0.1263 - val_acc: 0.9695\n",
      "Epoch 37/60\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0189 - acc: 0.9950 - val_loss: 0.1113 - val_acc: 0.9725\n",
      "Epoch 38/60\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0178 - acc: 0.9957 - val_loss: 0.1158 - val_acc: 0.9722\n",
      "Epoch 39/60\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0160 - acc: 0.9962 - val_loss: 0.1341 - val_acc: 0.9673\n",
      "Epoch 40/60\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0146 - acc: 0.9967 - val_loss: 0.1183 - val_acc: 0.9719\n",
      "Epoch 41/60\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0136 - acc: 0.9971 - val_loss: 0.1309 - val_acc: 0.9691\n",
      "Epoch 42/60\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0126 - acc: 0.9971 - val_loss: 0.2151 - val_acc: 0.9525\n",
      "Epoch 43/60\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0116 - acc: 0.9973 - val_loss: 0.1309 - val_acc: 0.9694\n",
      "Epoch 44/60\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0100 - acc: 0.9980 - val_loss: 0.1443 - val_acc: 0.9674\n",
      "Epoch 45/60\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0094 - acc: 0.9981 - val_loss: 0.1240 - val_acc: 0.9723\n",
      "Epoch 46/60\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0084 - acc: 0.9984 - val_loss: 0.1217 - val_acc: 0.9724\n",
      "Epoch 47/60\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0078 - acc: 0.9986 - val_loss: 0.1299 - val_acc: 0.9711\n",
      "Epoch 48/60\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0071 - acc: 0.9988 - val_loss: 0.1297 - val_acc: 0.9712\n",
      "Epoch 49/60\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0067 - acc: 0.9987 - val_loss: 0.1320 - val_acc: 0.9723\n",
      "Epoch 50/60\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0059 - acc: 0.9990 - val_loss: 0.1304 - val_acc: 0.9713\n",
      "Epoch 51/60\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0056 - acc: 0.9992 - val_loss: 0.1301 - val_acc: 0.9735\n",
      "Epoch 52/60\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0047 - acc: 0.9993 - val_loss: 0.1293 - val_acc: 0.9733\n",
      "Epoch 53/60\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0041 - acc: 0.9996 - val_loss: 0.1365 - val_acc: 0.9725\n",
      "Epoch 54/60\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0034 - acc: 0.9997 - val_loss: 0.1323 - val_acc: 0.9729\n",
      "Epoch 55/60\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0029 - acc: 0.9998 - val_loss: 0.1351 - val_acc: 0.9722\n",
      "Epoch 56/60\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0027 - acc: 0.9998 - val_loss: 0.1342 - val_acc: 0.9726\n",
      "Epoch 57/60\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0026 - acc: 0.9998 - val_loss: 0.1352 - val_acc: 0.9719\n",
      "Epoch 58/60\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 0.1348 - val_acc: 0.9725\n",
      "Epoch 59/60\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.1382 - val_acc: 0.9725\n",
      "Epoch 60/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0019 - acc: 0.9999 - val_loss: 0.1402 - val_acc: 0.9718\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "hist = model.fit(x_train, y_train, epochs=60, batch_size=64, validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 784)\n",
      "1\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "acc = hist.history['acc']\n",
    "val_acc = hist.history['val_acc']\n",
    "loss = hist.history['loss']\n",
    "val_loss = hist.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "mimg = load_img('/home/martin/fasdfas.png', target_size=(28,28), grayscale=True)\n",
    "mimg = image.img_to_array(mimg)\n",
    "mimg = 1-mimg.reshape((28, 28)).astype('float32') / 255.0\n",
    "\n",
    "mimg = mimg.reshape((1, 784))\n",
    "print(mimg.shape)\n",
    "pred=model.predict(mimg)\n",
    "print(np.argmax(pred))\n",
    "print(np.argmax(y_train[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8ed4dc1282ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mimg_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# note what number it is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0manswers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'f'"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "\n",
    "\n",
    "IMG_SIZE = (28, 28)\n",
    "\n",
    "JKJ\n",
    "\n",
    "reshaped_data = 1 - data.reshape(-1, 28*28)\n",
    "\n",
    "right = 0\n",
    "counter = [0]*10\n",
    "for p in range(len(answers)):\n",
    "    ans = model.predict(reshaped_data[p].reshape((1, 784)))\n",
    "    ans = np.argmax(ans)\n",
    "    print(\"{} : {}\".format(answers[p], ans))\n",
    "    if answers[p] == ans:\n",
    "        right += 1\n",
    "    counter[ans] += 1\n",
    "        \n",
    "print(right / len(answers))\n",
    "print(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=61\n",
    "img=x_train[n].reshape(IMG_SIZE)\n",
    "for r in img:\n",
    "    for c in r:\n",
    "        if c:\n",
    "            print('{: 2.0f}'.format(c*100), end='', sep='')\n",
    "        else:\n",
    "            print('  ', end='', sep='')\n",
    "    print()\n",
    "print(np.argmax(y_train[n]))\n",
    "\n",
    "img=1-data[52]\n",
    "for r in img:\n",
    "    for c in r:\n",
    "        if c:\n",
    "            print('{: 2.0f}'.format(c*100), end='', sep='')\n",
    "        else:\n",
    "            print('  ', end='', sep='')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
