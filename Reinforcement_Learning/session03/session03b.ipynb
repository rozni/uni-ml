{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TX00DQ05-3001 Exercises 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Sample behaviour of an MDP\n",
    "\n",
    "Let's take (again) a look at Sutton & Barto example 4.1 gridworld. On each iteration start at every (non-terminating) state and sample actions in succeeding states by selecting them from uniform distribution (each action - up, down, left, right - is equally probable). Run the episode until terminal state is encountered. Collect statistics to calculate average number of steps needed before completion for each start state. Should this number match with something you have seen earlier in the exercises?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common for all exercises\n",
    "STATES = [(i, j) for i in range(4) for j in range(4)]\n",
    "N_STATES = len(STATES)\n",
    "STATES_TERMINAL = [(0, 0), (3, 3)]\n",
    "STATES_NON_TERMINAL = [s for s in STATES if s not in STATES_TERMINAL]\n",
    "\n",
    "UP   = np.array((-1, 0))\n",
    "DOWN = np.array((1, 0))\n",
    "LEFT = np.array((0, -1))\n",
    "RIGHT= np.array((0, 1))\n",
    "\n",
    "ACTIONS = (UP, DOWN, LEFT, RIGHT)\n",
    "ACTION_LABELS = ('↑', '↓', '←', '→')\n",
    "N_ACTIONS = len(ACTIONS)\n",
    "\n",
    "def is_terminal(state):\n",
    "    return state in STATES_TERMINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "\n",
    "# a kinda general and slow approach here\n",
    "# but I thought was pretty - until I re-read the task\n",
    "# which says to count the steps, not the -1 reward\n",
    "# as in last week's exercises.\n",
    "# So I'll just flip the sign of the rewards\n",
    "# and will keep it this general...\n",
    "\n",
    "def take_action(state, action):\n",
    "    \"Returns reached state and associated reward\"\n",
    "    assert not is_terminal(state)\n",
    "    # treat action that would lead out of the grid as before\n",
    "    new_state = tuple(np.clip(state + action, 0, 3))\n",
    "    reward = 1 # a constant reward for counting steps\n",
    "    return new_state, reward\n",
    "\n",
    "def MC_pathlengths(maxiters):\n",
    "    lengths = dict()\n",
    "    # YOUR CODE\n",
    "    NON_TERM_STATES = [s for s in STATES if not is_terminal(s)]\n",
    "    returns = dict()\n",
    "    for i in range(maxiters):\n",
    "        # for each non-terminating state as starting state\n",
    "        for state in NON_TERM_STATES:\n",
    "            \n",
    "            # generate an episode\n",
    "            episode_states = list()\n",
    "            episode_rewards = list()\n",
    "            while not is_terminal(state):\n",
    "                episode_states.append(state)\n",
    "                # choose a random action uniformly\n",
    "                action = random.choice(ACTIONS)\n",
    "                state, reward = take_action(state, action)\n",
    "                episode_rewards.append(reward)\n",
    "                \n",
    "            # evaluate and improve expectations\n",
    "            G = 0\n",
    "            for t in reversed(range(len(episode_states))):\n",
    "                state = episode_states[t]\n",
    "                reward = episode_rewards[t]\n",
    "                \n",
    "                # non-discounted returns\n",
    "                G += reward\n",
    "                \n",
    "                if state not in episode_states[:t]:\n",
    "                    state_returns = returns.setdefault(state, list())\n",
    "                    state_returns.append(G)\n",
    "                    lengths[state] = np.mean(state_returns)\n",
    "\n",
    "    return lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\t14.18\t21.37\t23.27\t\n",
      "14.28\t19.32\t21.20\t20.73\t\n",
      "21.03\t20.84\t19.05\t14.88\t\n",
      "22.96\t21.43\t15.15\t-----\t\n"
     ]
    }
   ],
   "source": [
    "# it's rather slow, so just a few iterations\n",
    "lengths = MC_pathlengths(123)\n",
    "\n",
    "for row in range(4):\n",
    "    for col in range(4):\n",
    "        l = lengths.get((row, col))\n",
    "        s = f'{l:5.2f}' if l else '-----'\n",
    "        print(s, end='\\t')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is close (forgetting it's opposite) to the state values of random policy evaluation from 2nd session's exercises. Both are approximating the expected number of steps from a state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Monte Carlo state value function estimation. \n",
    "\n",
    "Calculate state-value function V for the gridworld of Sutton & Barto example 4.1 using first-visit or every-visit Monte Carlo policy evaluation (see for example page 92 of Sutton & Barto). Policy to be evaluated is the same as before; each action (up, down, left, right) is equally probable.  Action that would result in leaving the grid (for example moving up in top row) will leave state unchanged (but action has been taken). Gamma (discount factor) is assumed to be = 1, ie. no discounting.\n",
    "\n",
    "Try out both exploring starts (see Sutton & Barto, p. 96) and fixed start points. Any difference?\n",
    "\n",
    "Take a look at the value function you get when you run the algorithm multiple times (with fixed # of iterations). Any observations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "I'll try to answer the questions quantitatively instead of just talking..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slightly modified previous exercise\n",
    "\n",
    "def take_action(state, action):\n",
    "    \"Returns reached state and associated reward\"\n",
    "    assert not is_terminal(state)\n",
    "    new_state = tuple(np.clip(state + action, 0, 3))\n",
    "    reward = -1\n",
    "    return new_state, reward\n",
    "\n",
    "def MC(\n",
    "    maxiters: 'number of iterations, no early stopping',\n",
    "    every_visit: 'if true evaluate every visit, otherwise only first visit',\n",
    "    get_init_state: 'Callable() -> state',\n",
    ") -> 'state:value dictionary':\n",
    "    \n",
    "    V = dict()\n",
    "    mean_count = dict()\n",
    "    \n",
    "    for i in range(maxiters):\n",
    "\n",
    "        # generate an episode\n",
    "        episode_states = list()\n",
    "        episode_rewards = list()\n",
    "        state = get_init_state()\n",
    "        while not is_terminal(state):\n",
    "            episode_states.append(state)\n",
    "            # choose a random action uniformly\n",
    "            action = random.choice(ACTIONS)\n",
    "            state, reward = take_action(state, action)\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "        # evaluate and improve expectations\n",
    "        G = 0\n",
    "        for t in reversed(range(len(episode_states))):\n",
    "            state = episode_states[t]\n",
    "            reward = episode_rewards[t]\n",
    "\n",
    "            # non-discounted returns\n",
    "            G += reward\n",
    "\n",
    "            if every_visit or state not in episode_states[:t]:\n",
    "                mean = V.get(state, 0)\n",
    "                V[state] = mean + (G - mean)/mean_count.setdefault(state, 1)\n",
    "                mean_count[state] += 1\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(repeat, mc_args):\n",
    "    \"Reruns MC `repeat` times and prints statistics\"\n",
    "    results = np.zeros((repeat, 4, 4))\n",
    "    for i in range(repeat):\n",
    "        v = MC(*mc_args)\n",
    "        for state, value in v.items():\n",
    "            results[(i, *state)] = value\n",
    "            \n",
    "    std = results.std(axis=0)\n",
    "    std = std[std != 0] # only non-zero\n",
    "    print(f'Standard deviation: min={std.min():.2f}  max={std.max():.2f} avg={std.mean():.2f}')\n",
    "            \n",
    "    # a quick hack to print mean and std. dev. side-by-side\n",
    "    with np.printoptions(formatter={'float':\"{:6.2f}\".format}):\n",
    "        mean = str(results.mean(axis=0)).splitlines()\n",
    "        std =  str(results.std(axis=0)).splitlines()\n",
    "        \n",
    "    print(f'{\"Mean (over repeats)\":^27}  ¦  {\"Standard deviation\":^27}')\n",
    "    print('\\n'.join(\n",
    "        f'{m.strip(\" []\"):>27}  ¦  {s.strip(\" []\"):>27}'\n",
    "        for m, s in zip(mean, std)\n",
    "    ))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exploring states, first-visit, 4 iterations (repeated 100 times)\n",
      "Standard deviation: min=11.36  max=16.36 avg=13.21\n",
      "    Mean (over repeats)      ¦      Standard deviation     \n",
      "  0.00 -13.32 -18.22 -17.44  ¦    0.00  12.54  14.04  14.97\n",
      "-12.09 -17.48 -19.73 -18.03  ¦   13.48  11.49  12.30  14.71\n",
      "-17.81 -18.81 -16.51 -13.29  ¦   11.95  11.57  13.08  16.36\n",
      "-18.64 -17.08 -10.94   0.00  ¦   14.23  12.91  11.36   0.00\n",
      "\n",
      "Exploring states, every-visit, 4 iterations (repeated 100 times)\n",
      "Standard deviation: min=8.50  max=12.72 avg=10.73\n",
      "    Mean (over repeats)      ¦      Standard deviation     \n",
      "  0.00 -10.13 -13.23 -14.71  ¦    0.00  10.87  10.15  12.72\n",
      "-10.17 -13.76 -15.45 -14.40  ¦   12.45   9.75  10.50  12.66\n",
      "-13.82 -14.79 -13.31  -9.93  ¦   11.02   9.28   8.87  10.76\n",
      "-13.75 -13.06  -8.30   0.00  ¦   11.49  11.19   8.50   0.00\n",
      "\n",
      "Fixed state, first-visit, 4 iterations (repeated 100 times)\n",
      "Standard deviation: min=7.85  max=20.28 avg=16.82\n",
      "    Mean (over repeats)      ¦      Standard deviation     \n",
      "  0.00 -13.00 -17.19 -13.74  ¦    0.00  15.86  18.78  20.28\n",
      "-15.59 -20.84 -19.34 -16.22  ¦    7.85  14.46  18.70  19.95\n",
      "-22.04 -22.64 -15.74 -11.27  ¦   16.73  17.32  15.63  16.91\n",
      "-19.69 -18.25 -11.59   0.00  ¦   18.45  18.60  16.02   0.00\n",
      "\n",
      "Fixed state, every-visit, 4 iterations (repeated 100 times)\n",
      "Standard deviation: min=8.13  max=15.74 avg=11.85\n",
      "    Mean (over repeats)      ¦      Standard deviation     \n",
      "  0.00  -9.64 -10.62 -10.26  ¦    0.00  12.64  12.54  14.76\n",
      "-11.52 -12.33 -10.83 -10.10  ¦    8.13   8.88  10.21  12.94\n",
      "-13.44 -12.34  -9.90  -6.22  ¦   11.72  10.63  12.86  10.92\n",
      "-13.02 -11.20  -5.99   0.00  ¦   15.74  13.48  10.46   0.00\n",
      "\n",
      "============================================================\n",
      "\n",
      "Exploring states, first-visit, 16 iterations (repeated 100 times)\n",
      "Standard deviation: min=6.29  max=11.18 avg=8.13\n",
      "    Mean (over repeats)      ¦      Standard deviation     \n",
      "  0.00 -14.09 -19.56 -21.26  ¦    0.00   7.81  11.18   9.48\n",
      "-14.01 -18.29 -19.34 -19.22  ¦    6.29   8.91   7.88   8.04\n",
      "-20.37 -19.76 -17.68 -14.59  ¦    9.66   6.65   6.31   7.65\n",
      "-22.42 -19.62 -13.26   0.00  ¦    9.91   7.32   6.77   0.00\n",
      "\n",
      "Exploring states, every-visit, 16 iterations (repeated 100 times)\n",
      "Standard deviation: min=6.33  max=10.48 avg=7.73\n",
      "    Mean (over repeats)      ¦      Standard deviation     \n",
      "  0.00 -14.19 -19.60 -20.49  ¦    0.00  10.48   9.65   9.37\n",
      "-13.60 -16.97 -19.07 -19.01  ¦    7.80   7.59   7.98   7.56\n",
      "-17.62 -17.89 -16.51 -13.69  ¦    6.89   6.33   6.69   7.59\n",
      "-18.62 -17.83 -12.59   0.00  ¦    7.13   6.84   6.33   0.00\n",
      "\n",
      "Fixed state, first-visit, 16 iterations (repeated 100 times)\n",
      "Standard deviation: min=3.94  max=12.95 avg=8.66\n",
      "    Mean (over repeats)      ¦      Standard deviation     \n",
      "  0.00 -12.53 -18.46 -19.55  ¦    0.00   8.58   9.96  12.95\n",
      "-13.65 -18.02 -18.88 -19.21  ¦    3.94   7.09   8.10  11.63\n",
      "-20.40 -20.34 -17.94 -13.20  ¦    7.04   7.83   7.88   8.60\n",
      "-22.76 -19.98 -13.65   0.00  ¦   10.04   8.61   9.00   0.00\n",
      "\n",
      "Fixed state, every-visit, 16 iterations (repeated 100 times)\n",
      "Standard deviation: min=5.21  max=14.72 avg=9.15\n",
      "    Mean (over repeats)      ¦      Standard deviation     \n",
      "  0.00 -12.82 -16.09 -17.45  ¦    0.00   8.89  10.74  14.72\n",
      "-13.06 -16.81 -18.17 -17.35  ¦    5.21   7.07   9.38  12.67\n",
      "-17.82 -17.93 -16.89 -13.13  ¦    7.20   6.37   8.44  10.63\n",
      "-17.68 -17.81 -13.52   0.00  ¦    8.19   8.74   9.81   0.00\n",
      "\n",
      "============================================================\n",
      "\n",
      "Exploring states, first-visit, 64 iterations (repeated 100 times)\n",
      "Standard deviation: min=2.74  max=4.00 avg=3.45\n",
      "    Mean (over repeats)      ¦      Standard deviation     \n",
      "  0.00 -13.56 -19.44 -21.44  ¦    0.00   2.74   3.27   3.72\n",
      "-14.27 -18.35 -19.72 -19.64  ¦    3.63   3.38   3.28   3.49\n",
      "-20.09 -20.26 -17.98 -14.10  ¦    3.76   3.46   2.97   3.53\n",
      "-21.72 -20.05 -13.82   0.00  ¦    4.00   3.77   3.25   0.00\n",
      "\n",
      "Exploring states, every-visit, 64 iterations (repeated 100 times)\n",
      "Standard deviation: min=3.46  max=4.93 avg=3.93\n",
      "    Mean (over repeats)      ¦      Standard deviation     \n",
      "  0.00 -13.70 -18.94 -21.14  ¦    0.00   4.10   4.23   4.49\n",
      "-14.01 -17.93 -19.22 -18.89  ¦    3.73   3.46   3.81   3.58\n",
      "-20.11 -20.14 -17.61 -13.47  ¦    3.73   3.52   3.73   3.65\n",
      "-22.37 -20.24 -13.61   0.00  ¦    4.93   4.29   3.83   0.00\n",
      "\n",
      "Fixed state, first-visit, 64 iterations (repeated 100 times)\n",
      "Standard deviation: min=2.29  max=6.33 avg=4.30\n",
      "    Mean (over repeats)      ¦      Standard deviation     \n",
      "  0.00 -13.93 -20.35 -22.40  ¦    0.00   3.89   5.03   6.33\n",
      "-14.17 -18.09 -19.67 -20.24  ¦    2.29   3.13   4.39   5.68\n",
      "-20.01 -20.12 -18.09 -14.40  ¦    3.19   3.52   4.04   5.10\n",
      "-22.09 -19.68 -13.81   0.00  ¦    4.52   4.44   4.60   0.00\n",
      "\n",
      "Fixed state, every-visit, 64 iterations (repeated 100 times)\n",
      "Standard deviation: min=2.67  max=8.50 avg=5.21\n",
      "    Mean (over repeats)      ¦      Standard deviation     \n",
      "  0.00 -14.39 -19.27 -21.67  ¦    0.00   4.73   5.71   8.50\n",
      "-13.92 -18.05 -19.67 -19.28  ¦    2.67   4.11   5.18   6.52\n",
      "-19.59 -19.30 -17.46 -13.64  ¦    4.37   4.60   5.33   5.65\n",
      "-20.40 -18.90 -13.42   0.00  ¦    4.91   5.19   5.52   0.00\n",
      "\n",
      "============================================================\n",
      "\n",
      "Exploring states, first-visit, 256 iterations (repeated 100 times)\n",
      "Standard deviation: min=1.56  max=2.17 avg=1.79\n",
      "    Mean (over repeats)      ¦      Standard deviation     \n",
      "  0.00 -14.19 -20.27 -22.13  ¦    0.00   1.71   1.87   2.17\n",
      "-13.93 -17.91 -19.92 -20.14  ¦    1.69   1.70   1.67   2.03\n",
      "-19.90 -19.77 -17.86 -14.01  ¦    1.83   1.74   1.65   1.71\n",
      "-22.07 -19.75 -13.85   0.00  ¦    1.94   1.82   1.56   0.00\n",
      "\n",
      "Exploring states, every-visit, 256 iterations (repeated 100 times)\n",
      "Standard deviation: min=1.82  max=2.54 avg=2.14\n",
      "    Mean (over repeats)      ¦      Standard deviation     \n",
      "  0.00 -13.84 -19.62 -21.74  ¦    0.00   1.90   2.25   2.54\n",
      "-13.83 -17.76 -19.80 -19.75  ¦    2.17   1.86   2.02   2.42\n",
      "-19.72 -19.60 -17.82 -13.67  ¦    2.42   1.95   1.82   1.94\n",
      "-21.80 -19.86 -13.96   0.00  ¦    2.40   2.29   1.93   0.00\n",
      "\n",
      "Fixed state, first-visit, 256 iterations (repeated 100 times)\n",
      "Standard deviation: min=1.18  max=2.72 avg=2.04\n",
      "    Mean (over repeats)      ¦      Standard deviation     \n",
      "  0.00 -13.90 -19.96 -21.86  ¦    0.00   1.81   2.37   2.72\n",
      "-14.07 -17.96 -19.92 -19.97  ¦    1.18   1.57   2.14   2.46\n",
      "-19.92 -20.11 -18.06 -13.86  ¦    1.75   1.85   2.04   2.43\n",
      "-22.01 -20.13 -14.13   0.00  ¦    2.07   2.15   2.06   0.00\n",
      "\n",
      "Fixed state, every-visit, 256 iterations (repeated 100 times)\n",
      "Standard deviation: min=1.22  max=3.93 avg=2.45\n",
      "    Mean (over repeats)      ¦      Standard deviation     \n",
      "  0.00 -13.89 -19.65 -21.58  ¦    0.00   2.27   3.03   3.93\n",
      "-13.85 -18.00 -19.91 -19.81  ¦    1.22   1.80   2.38   3.08\n",
      "-19.86 -20.08 -17.80 -13.72  ¦    2.00   1.81   2.21   2.70\n",
      "-21.85 -20.02 -14.01   0.00  ¦    2.78   2.54   2.50   0.00\n",
      "\n",
      "============================================================\n",
      "\n",
      "Exploring states, first-visit, 1024 iterations (repeated 100 times)\n",
      "Standard deviation: min=0.75  max=0.97 avg=0.85\n",
      "    Mean (over repeats)      ¦      Standard deviation     \n",
      "  0.00 -14.11 -20.06 -22.08  ¦    0.00   0.88   0.89   0.97\n",
      "-14.06 -18.11 -20.04 -20.09  ¦    0.85   0.82   0.76   0.95\n",
      "-20.07 -20.05 -18.06 -14.13  ¦    0.83   0.78   0.75   0.83\n",
      "-21.98 -20.08 -14.05   0.00  ¦    0.94   0.84   0.77   0.00\n",
      "\n",
      "Exploring states, every-visit, 1024 iterations (repeated 100 times)\n",
      "Standard deviation: min=0.97  max=1.38 avg=1.11\n",
      "    Mean (over repeats)      ¦      Standard deviation     \n",
      "  0.00 -13.86 -19.74 -21.75  ¦    0.00   1.20   1.25   1.38\n",
      "-13.98 -17.87 -19.85 -19.82  ¦    1.01   0.97   1.07   1.22\n",
      "-19.91 -19.87 -17.94 -13.87  ¦    1.09   1.03   1.01   1.06\n",
      "-21.84 -19.82 -13.84   0.00  ¦    1.20   1.06   1.02   0.00\n",
      "\n",
      "Fixed state, first-visit, 1024 iterations (repeated 100 times)\n",
      "Standard deviation: min=0.50  max=1.40 avg=0.96\n",
      "    Mean (over repeats)      ¦      Standard deviation     \n",
      "  0.00 -13.93 -19.94 -21.93  ¦    0.00   0.88   1.07   1.40\n",
      "-13.99 -17.94 -19.97 -19.92  ¦    0.50   0.73   0.93   1.21\n",
      "-20.03 -20.02 -17.94 -13.91  ¦    0.80   0.81   0.96   1.13\n",
      "-22.01 -20.02 -14.03   0.00  ¦    1.04   0.98   1.07   0.00\n",
      "\n",
      "Fixed state, every-visit, 1024 iterations (repeated 100 times)\n",
      "Standard deviation: min=0.50  max=1.89 avg=1.16\n",
      "    Mean (over repeats)      ¦      Standard deviation     \n",
      "  0.00 -13.82 -19.98 -21.93  ¦    0.00   1.15   1.48   1.89\n",
      "-13.85 -17.82 -19.85 -19.81  ¦    0.50   0.84   1.21   1.54\n",
      "-19.85 -19.91 -17.90 -13.87  ¦    0.84   0.86   0.98   1.21\n",
      "-21.83 -19.88 -13.98   0.00  ¦    1.30   1.18   1.23   0.00\n",
      "\n",
      "============================================================\n",
      "\n",
      "Exploring states, first-visit, 4096 iterations (repeated 100 times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation: min=0.34  max=0.50 avg=0.39\n",
      "    Mean (over repeats)      ¦      Standard deviation     \n",
      "  0.00 -13.93 -19.94 -21.99  ¦    0.00   0.35   0.41   0.50\n",
      "-13.98 -17.94 -19.97 -20.00  ¦    0.38   0.34   0.36   0.44\n",
      "-19.99 -19.99 -18.01 -14.02  ¦    0.39   0.38   0.36   0.38\n",
      "-21.96 -19.97 -14.03   0.00  ¦    0.41   0.41   0.38   0.00\n",
      "\n",
      "Exploring states, every-visit, 4096 iterations (repeated 100 times)\n",
      "Standard deviation: min=0.46  max=0.64 avg=0.54\n",
      "    Mean (over repeats)      ¦      Standard deviation     \n",
      "  0.00 -14.11 -20.08 -22.09  ¦    0.00   0.61   0.61   0.64\n",
      "-14.04 -18.06 -20.07 -20.10  ¦    0.54   0.47   0.49   0.60\n",
      "-19.99 -20.03 -18.04 -14.10  ¦    0.54   0.48   0.46   0.52\n",
      "-22.03 -20.00 -14.04   0.00  ¦    0.62   0.52   0.49   0.00\n",
      "\n",
      "Fixed state, first-visit, 4096 iterations (repeated 100 times)\n",
      "Standard deviation: min=0.27  max=0.76 avg=0.53\n",
      "    Mean (over repeats)      ¦      Standard deviation     \n",
      "  0.00 -14.03 -19.96 -22.01  ¦    0.00   0.50   0.62   0.76\n",
      "-13.99 -17.99 -20.01 -20.05  ¦    0.27   0.40   0.50   0.68\n",
      "-19.99 -19.98 -17.98 -14.01  ¦    0.43   0.45   0.50   0.59\n",
      "-21.97 -19.97 -13.95   0.00  ¦    0.59   0.57   0.53   0.00\n",
      "\n",
      "Fixed state, every-visit, 4096 iterations (repeated 100 times)\n",
      "Standard deviation: min=0.34  max=0.88 avg=0.64\n",
      "    Mean (over repeats)      ¦      Standard deviation     \n",
      "  0.00 -14.01 -19.97 -21.96  ¦    0.00   0.66   0.74   0.88\n",
      "-13.93 -17.94 -19.99 -19.95  ¦    0.34   0.52   0.59   0.76\n",
      "-19.92 -19.96 -17.95 -13.93  ¦    0.58   0.60   0.57   0.67\n",
      "-22.08 -20.07 -14.02   0.00  ¦    0.71   0.73   0.66   0.00\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# try these iterations\n",
    "ITERS = (4, 16, 64, 256, 1024, 4096)\n",
    "# this many times\n",
    "REPEAT = 100\n",
    "\n",
    "get_init_state_explore = lambda: random.choice(STATES_NON_TERMINAL)\n",
    "get_init_state_fixed = lambda: (1,0)\n",
    "\n",
    "for iters in ITERS:\n",
    "    print(f\"\\nExploring states, first-visit, {iters} iterations (repeated {REPEAT} times)\")\n",
    "    stats(REPEAT, (iters, False, get_init_state_explore))\n",
    "    \n",
    "    print(f\"\\nExploring states, every-visit, {iters} iterations (repeated {REPEAT} times)\")\n",
    "    stats(REPEAT, (iters, True, get_init_state_explore))\n",
    "    \n",
    "    print(f\"\\nFixed state, first-visit, {iters} iterations (repeated {REPEAT} times)\")\n",
    "    stats(REPEAT, (iters, False, get_init_state_fixed))\n",
    "    \n",
    "    print(f\"\\nFixed state, every-visit, {iters} iterations (repeated {REPEAT} times)\")\n",
    "    stats(REPEAT, (iters, True, get_init_state_fixed))\n",
    "    \n",
    "    print('\\n', '='*60, sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3*: Monte Carlo action value function estimation\n",
    "\n",
    "Use the same idea as in exercise 2 to estimate q function.\n",
    "\n",
    "*) - not mandatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCq(\n",
    "    maxiters: 'number of iterations, no early stopping',\n",
    "    every_visit: 'if true evaluate every visit, otherwise only first visit',\n",
    "    get_init_state: 'Callable() -> state',\n",
    ") -> 'action-value table - 2D array: state_indices×action_indices':\n",
    "    \n",
    "    Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "    mean_count = np.ones_like(Q)\n",
    "    \n",
    "    for i in range(maxiters):\n",
    "\n",
    "        # generate an episode\n",
    "        episode = list()\n",
    "        state = get_init_state()\n",
    "        while not is_terminal(state):\n",
    "            action = np.random.randint(N_ACTIONS)\n",
    "            new_state, reward = take_action(state, ACTIONS[action])\n",
    "            \n",
    "            state_index = STATES.index(state)\n",
    "            episode.append((state_index, action , reward))\n",
    "            state = new_state\n",
    "\n",
    "        # evaluate and improve expectations\n",
    "        G = 0\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            \n",
    "            # non-discounted returns\n",
    "            G += reward\n",
    "\n",
    "            prior_sa_pairs = ((s,a) for s, a, r in episode[:t])\n",
    "            if every_visit or ((state, action) not in prior_sa_pairs):\n",
    "                mean = Q[state, action]\n",
    "                Q[state, action] = mean + (G - mean)/mean_count[state, action]\n",
    "                mean_count[state, action] += 1\n",
    "                \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_action_value(Q):\n",
    "    print('Action-value table:')\n",
    "    print('State ' + ('  {:>4}  '*4).format(*ACTION_LABELS))\n",
    "    for i in range(len(STATES)):\n",
    "        print('{}  {:6.2f}  {:6.2f}  {:6.2f}  {:6.2f}'.format(STATES[i], *Q[i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action-value table:\n",
      "State      ↑       ↓       ←       →  \n",
      "(0, 0)    0.00    0.00    0.00    0.00\n",
      "(0, 1)  -14.90  -18.82   -1.00  -20.99\n",
      "(0, 2)  -20.99  -21.15  -14.81  -23.13\n",
      "(0, 3)  -22.74  -21.19  -21.02  -22.85\n",
      "(1, 0)   -1.00  -20.93  -14.85  -19.68\n",
      "(1, 1)  -15.45  -21.26  -15.16  -20.80\n",
      "(1, 2)  -21.14  -19.06  -18.85  -20.76\n",
      "(1, 3)  -23.03  -14.82  -21.25  -21.43\n",
      "(2, 0)  -15.27  -22.65  -21.06  -20.81\n",
      "(2, 1)  -19.00  -20.90  -21.04  -19.37\n",
      "(2, 2)  -20.77  -14.91  -21.16  -14.97\n",
      "(2, 3)  -21.06   -1.00  -18.88  -14.12\n",
      "(3, 0)  -21.01  -22.92  -22.81  -20.94\n",
      "(3, 1)  -21.47  -20.76  -23.17  -14.93\n",
      "(3, 2)  -18.78  -15.07  -21.18   -1.00\n",
      "(3, 3)    0.00    0.00    0.00    0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explore starts, first visit\n",
    "q = MCq(12345, False, get_init_state_explore)\n",
    "print_action_value(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4*: Monte Carlo control\n",
    "\n",
    "Compute the optimal policy for the 4x4 gridworld example. Start with random policy. Consider the epsilon adjustment schedule - can it in practise be 1/k, or is something more conservative better? Can you think of any other tricks to manage the noisiness of MC?\n",
    "\n",
    "*) - not mandatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCC(\n",
    "    maxiters: 'number of iterations, no early stopping',\n",
    "    every_visit: 'if true evaluate every visit, otherwise only first visit',\n",
    "    get_init_state: 'Callable() -> state',\n",
    "    policy: 'Callable(q, state_index, eps) -> action_index',\n",
    "    eps_schedule: 'Callable(iteration, previous_eps?) -> eps',\n",
    ") -> 'action-value table - 2D array: state_indices×action_indices':\n",
    "    \n",
    "    Q = np.zeros((N_STATES, N_ACTIONS))\n",
    "    mean_count = np.ones_like(Q)\n",
    "    eps = None\n",
    "    \n",
    "    for i in range(1, maxiters+1):\n",
    "\n",
    "        # generate an episode\n",
    "        episode = list()\n",
    "        state = get_init_state()\n",
    "        eps = eps_schedule(i, eps)\n",
    "        while not is_terminal(state):\n",
    "            state_index = STATES.index(state)\n",
    "            \n",
    "            action = policy(Q, state_index, eps)\n",
    "            new_state, reward = take_action(state, ACTIONS[action])\n",
    "            \n",
    "            episode.append((state_index, action , reward))\n",
    "            state = new_state\n",
    "\n",
    "        # evaluate and improve expectations\n",
    "        G = 0\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            \n",
    "            # non-discounted returns\n",
    "            G += reward\n",
    "\n",
    "            prior_sa_pairs = ((s,a) for s, a, r in episode[:t])\n",
    "            if every_visit or ((state, action) not in prior_sa_pairs):\n",
    "                mean = Q[state, action]\n",
    "                Q[state, action] = mean + (G - mean)/mean_count[state, action]\n",
    "                mean_count[state, action] += 1\n",
    "                \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy_policy(q, state_index, eps):\n",
    "    \"\"\"\n",
    "    eps=0 for greedy action,\n",
    "    eps=1 for random action\n",
    "    \n",
    "    returns action index\"\"\"\n",
    "    if np.random.random() < eps:\n",
    "        return np.random.randint(N_ACTIONS)\n",
    "    else:\n",
    "        return np.argmax(q[state_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_resulting_greedy_policy(Q):\n",
    "    print('Resulting greedy policy:')\n",
    "    for state_index in range(N_STATES):\n",
    "        action_label = '×'\n",
    "\n",
    "        if not is_terminal(STATES[state_index]):\n",
    "            action_index = np.argmax(Q[state_index])\n",
    "            action_label = ACTION_LABELS[action_index]\n",
    "\n",
    "        print(action_label, end=' ')\n",
    "        if state_index % 4 == 3:\n",
    "            print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action-value table:\n",
      "State      ↑       ↓       ←       →  \n",
      "(0, 0)    0.00    0.00    0.00    0.00\n",
      "(0, 1)   -7.67  -17.00   -1.00  -46.00\n",
      "(0, 2)  -57.00  -36.00   -2.06  -53.00\n",
      "(0, 3)  -25.67  -32.00   -3.11  -20.50\n",
      "(1, 0)   -1.00  -74.00  -26.00  -53.00\n",
      "(1, 1)  -16.00   -4.04  -75.00  -39.00\n",
      "(1, 2)  -58.00   -3.00   -5.00  -42.33\n",
      "(1, 3)  -22.67  -28.00   -4.18  -72.00\n",
      "(2, 0)   -2.00  -18.33  -41.00  -72.00\n",
      "(2, 1)  -69.00  -71.00   -3.01  -23.00\n",
      "(2, 2)  -61.00  -22.00   -4.00   -2.00\n",
      "(2, 3)  -27.00   -1.00   -4.00  -11.50\n",
      "(3, 0)   -3.01  -29.50  -22.00  -38.00\n",
      "(3, 1)  -70.00  -10.50   -4.00   -2.00\n",
      "(3, 2)  -21.00   -2.50  -19.00   -1.00\n",
      "(3, 3)    0.00    0.00    0.00    0.00\n",
      "\n",
      "Resulting greedy policy:\n",
      "× ← ← ← \n",
      "↑ ↓ ↓ ← \n",
      "↑ ← → ↓ \n",
      "↑ → → × \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# eps - 1/i, explore starts, first visit\n",
    "def eps_schedule(iteration, prev_eps):\n",
    "    return 1/iteration\n",
    "\n",
    "q = MCC(5000, False, get_init_state_explore, eps_greedy_policy, eps_schedule)\n",
    "print_action_value(q)\n",
    "print_resulting_greedy_policy(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schedule is way too agressive - i.e. very slow exploration and thus convergence. For this problem, schedule 100/i was already able do find optimal policy often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action-value table:\n",
      "State      ↑       ↓       ←       →  \n",
      "(0, 0)    0.00    0.00    0.00    0.00\n",
      "(0, 1)   -5.72   -5.10   -1.00   -7.93\n",
      "(0, 2)   -9.35   -4.44   -5.73   -8.96\n",
      "(0, 3)  -12.92  -10.71   -5.55  -11.36\n",
      "(1, 0)   -1.00   -5.32   -2.26   -3.42\n",
      "(1, 1)   -2.43   -8.23   -6.88  -10.06\n",
      "(1, 2)   -6.58   -3.33   -5.39   -5.35\n",
      "(1, 3)  -13.71   -2.41   -8.43   -9.38\n",
      "(2, 0)   -2.25   -7.20   -6.00   -7.30\n",
      "(2, 1)   -8.86   -7.79   -3.44   -8.11\n",
      "(2, 2)   -4.72   -2.18   -4.34   -3.87\n",
      "(2, 3)   -8.70   -1.00   -3.39   -6.13\n",
      "(3, 0)   -6.64   -4.82   -7.94   -3.27\n",
      "(3, 1)   -6.07   -5.71   -6.95   -2.22\n",
      "(3, 2)   -3.26   -2.19   -3.95   -1.00\n",
      "(3, 3)    0.00    0.00    0.00    0.00\n",
      "\n",
      "Resulting greedy policy:\n",
      "× ← ↓ ← \n",
      "↑ ↑ ↓ ↓ \n",
      "↑ ← ↓ ↓ \n",
      "→ → → × \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# eps - logarithmic decay, explore starts, first visit\n",
    "def eps_schedule(iteration, prev_eps):\n",
    "    return 1/(1 + np.log(iteration))\n",
    "\n",
    "q = MCC(5000, False, get_init_state_explore, eps_greedy_policy, eps_schedule)\n",
    "print_action_value(q)\n",
    "print_resulting_greedy_policy(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much slower decay in the long run, although drops quite quickly in the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action-value table:\n",
      "State      ↑       ↓       ←       →  \n",
      "(0, 0)    0.00    0.00    0.00    0.00\n",
      "(0, 1)   -4.00  -12.77   -1.00   -8.23\n",
      "(0, 2)  -12.50  -14.08   -2.12  -12.64\n",
      "(0, 3)  -11.12  -10.29   -3.35  -10.40\n",
      "(1, 0)   -1.00  -11.64   -3.08   -9.50\n",
      "(1, 1)   -9.33  -10.83   -2.10  -13.33\n",
      "(1, 2)  -13.40  -13.88   -3.44  -14.11\n",
      "(1, 3)  -14.63   -2.12  -12.22  -11.20\n",
      "(2, 0)   -2.13  -18.00  -11.40  -15.15\n",
      "(2, 1)   -3.38  -17.27  -17.36  -16.40\n",
      "(2, 2)  -11.27   -2.34  -15.80  -13.17\n",
      "(2, 3)   -8.82   -1.00  -14.50   -5.60\n",
      "(3, 0)   -3.19  -15.70  -13.17  -13.06\n",
      "(3, 1)  -16.18  -18.45   -4.44  -15.27\n",
      "(3, 2)  -11.67   -9.82  -18.80   -1.00\n",
      "(3, 3)    0.00    0.00    0.00    0.00\n",
      "\n",
      "Resulting greedy policy:\n",
      "× ← ← ← \n",
      "↑ ← ← ↓ \n",
      "↑ ↑ ↓ ↓ \n",
      "↑ ← → × \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# eps - exponential decay, explore starts, first visit\n",
    "def eps_schedule(iteration, prev_eps):\n",
    "    prev_eps = prev_eps or 1\n",
    "    return 0.99*prev_eps\n",
    "\n",
    "q = MCC(5000, False, get_init_state_explore, eps_greedy_policy, eps_schedule)\n",
    "print_action_value(q)\n",
    "print_resulting_greedy_policy(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although exponential decay approaches zero much faster in the limit, it has a plenty of time to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action-value table:\n",
      "State      ↑       ↓       ←       →  \n",
      "(0, 0)    0.00    0.00    0.00    0.00\n",
      "(0, 1)   -3.22   -3.93   -1.00   -5.03\n",
      "(0, 2)   -6.25   -5.71   -2.73   -7.07\n",
      "(0, 3)   -7.19   -4.50   -6.47   -7.34\n",
      "(1, 0)   -1.00   -4.43   -2.73   -3.98\n",
      "(1, 1)   -2.68   -5.41   -2.91   -5.40\n",
      "(1, 2)   -6.05   -4.00   -4.39   -4.98\n",
      "(1, 3)   -6.46   -2.65   -5.28   -4.68\n",
      "(2, 0)   -2.62   -5.71   -5.24   -5.29\n",
      "(2, 1)   -4.48   -5.23   -4.68   -3.95\n",
      "(2, 2)   -4.90   -3.08   -5.32   -2.63\n",
      "(2, 3)   -4.58   -1.00   -4.11   -2.81\n",
      "(3, 0)   -5.33   -5.32   -5.66   -4.04\n",
      "(3, 1)   -5.28   -4.61   -5.69   -2.57\n",
      "(3, 2)   -4.48   -2.68   -4.15   -1.00\n",
      "(3, 3)    0.00    0.00    0.00    0.00\n",
      "\n",
      "Resulting greedy policy:\n",
      "× ← ← ↓ \n",
      "↑ ↑ ↓ ↓ \n",
      "↑ → → ↓ \n",
      "→ → → × \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# eps - constant, explore starts, first visit\n",
    "def eps_schedule(iteration, prev_eps):\n",
    "    return 0.3\n",
    "\n",
    "q = MCC(5000, False, get_init_state_explore, eps_greedy_policy, eps_schedule)\n",
    "print_action_value(q)\n",
    "print_resulting_greedy_policy(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not GLIE but still works here.\n",
    "\n",
    "---\n",
    "\n",
    "The MC noisiness could be mitigated for example by updating the action values more conservatively (putting less weight on single rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
