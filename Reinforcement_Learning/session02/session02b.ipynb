{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TX00DQ05-3001 Exercises 2\n",
    "\n",
    "Note that you don't have to use the functions / other code in the cells below. They are there just in case you need inspiration to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n",
    "import numpy.linalg as LA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Iterative policy evaluation. \n",
    "\n",
    "Calculate state-value function V for the gridworld of Sutton & Barto example 4.1. Policy is assumed to be random, ie. each of the four directions are equally likely. Movement that would result in leaving the grid (for example moving up in top row) will leave state unchanged (but action has been taken). Gamma (discount factor) is assumed to be = 1, ie. no discounting.\n",
    "\n",
    "When norm of the difference between new V and the old one is less than eps, stop iteration.\n",
    "\n",
    "Compare needed number of iterations between synchronous (sweep over all states, and update value function after the sweep) and asynchronous (use always the latest values) update of state-value function.\n",
    "\n",
    "Note that numpy tensor assignment does not create a copy. You might want to use .copy() method to avoid sharing a reference to the same array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP   = np.array((-1, 0))\n",
    "DOWN = np.array((1, 0))\n",
    "LEFT = np.array((0, -1))\n",
    "RIGHT= np.array((0, 1))\n",
    "\n",
    "DIRECTIONS = (UP, DOWN, LEFT, RIGHT)\n",
    "DIRECTIONS_LABELS = ('↑', '↓', '←', '→')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 319\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n"
     ]
    }
   ],
   "source": [
    "# fully synchronous\n",
    "rows_count = 4\n",
    "columns_count = 4\n",
    "V = np.zeros((rows_count, columns_count))\n",
    "V_new = np.zeros((rows_count, columns_count))\n",
    "terminating = [(0,0), (rows_count-1, columns_count-1)]\n",
    "stepcost = -1\n",
    "\n",
    "maxiters = 1000\n",
    "eps = 0.0000001\n",
    "\n",
    "def take_action(row, column, direction):\n",
    "    return tuple(np.clip((row, column) + direction, 0, 3))\n",
    "\n",
    "for i in range(maxiters):\n",
    "    V = V_new.copy()\n",
    "    for row in range(rows_count):\n",
    "        for col in range(columns_count):\n",
    "            if (row, col) in terminating:\n",
    "                continue\n",
    "            V_new[row, col] = sum((-1 + V[take_action(row, col, action)] for action in DIRECTIONS)) / 4\n",
    "    \n",
    "    if LA.norm(V - V_new) < eps:\n",
    "        break\n",
    "\n",
    "print(\"iteration\", i)\n",
    "with np.printoptions(precision=3):\n",
    "    print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 205\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n"
     ]
    }
   ],
   "source": [
    "# on way between synchronous and asynchronous\n",
    "# (V_new assignment and summing)\n",
    "rows_count = 4\n",
    "columns_count = 4\n",
    "V = np.zeros((rows_count, columns_count))\n",
    "V_new = np.zeros((rows_count, columns_count))\n",
    "terminating = [(0,0), (rows_count-1, columns_count-1)]\n",
    "stepcost = -1\n",
    "\n",
    "maxiters = 1000\n",
    "eps = 0.0000001\n",
    "\n",
    "def take_action(row, column, direction):\n",
    "    return tuple(np.clip((row, column) + direction, 0, 3))\n",
    "\n",
    "for i in range(maxiters):\n",
    "    V = V_new.copy()\n",
    "    for row in range(rows_count):\n",
    "        for col in range(columns_count):\n",
    "            if (row, col) in terminating:\n",
    "                continue\n",
    "            V_new[row, col] = sum((-1 + V_new[take_action(row, col, action)] for action in DIRECTIONS)) / 4\n",
    "    \n",
    "    if LA.norm(V - V_new) < eps:\n",
    "        break\n",
    "\n",
    "print(\"iteration\", i)\n",
    "with np.printoptions(precision=3):\n",
    "    print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 185\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n"
     ]
    }
   ],
   "source": [
    "# asynchronous\n",
    "rows_count = 4\n",
    "columns_count = 4\n",
    "V = np.zeros((rows_count, columns_count))\n",
    "V_new = np.zeros((rows_count, columns_count))\n",
    "terminating = [(0,0), (rows_count-1, columns_count-1)]\n",
    "stepcost = -1\n",
    "\n",
    "maxiters = 1000\n",
    "eps = 0.0000001\n",
    "\n",
    "def take_action(row, column, direction):\n",
    "    return tuple(np.clip((row, column) + direction, 0, 3))\n",
    "\n",
    "for i in range(maxiters):\n",
    "    for row in range(rows_count):\n",
    "        for col in range(columns_count):\n",
    "            if (row, col) in terminating:\n",
    "                continue\n",
    "            V = V_new.copy()\n",
    "            V_new[row, col] = sum((-1 + V[take_action(row, col, action)] for action in DIRECTIONS)) / 4\n",
    "    \n",
    "    if LA.norm(V - V_new) < eps:\n",
    "        break\n",
    "\n",
    "print(\"iteration\", i)\n",
    "with np.printoptions(precision=3):\n",
    "    print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 185\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n"
     ]
    }
   ],
   "source": [
    "# asynchronous\n",
    "rows_count = 4\n",
    "columns_count = 4\n",
    "V = np.zeros((rows_count, columns_count))\n",
    "V_new = np.zeros((rows_count, columns_count))\n",
    "terminating = [(0,0), (rows_count-1, columns_count-1)]\n",
    "stepcost = -1\n",
    "\n",
    "maxiters = 1000\n",
    "eps = 0.0000001\n",
    "\n",
    "def take_action(row, column, direction):\n",
    "    return tuple(np.clip((row, column) + direction, 0, 3))\n",
    "\n",
    "for i in range(maxiters):\n",
    "    for row in range(rows_count):\n",
    "        for col in range(columns_count):\n",
    "            if (row, col) in terminating:\n",
    "                continue\n",
    "            V = V_new.copy()\n",
    "            V_new[row, col] = sum((-1 + V[take_action(row, col, action)] for action in DIRECTIONS)) / 4\n",
    "    \n",
    "    if LA.norm(V - V_new) < eps:\n",
    "        break\n",
    "\n",
    "print(\"iteration\", i)\n",
    "with np.printoptions(precision=3):\n",
    "    print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Greedy policy. \n",
    "\n",
    "Based on the state-value function computed in exercise 1, print out deterministic greedy policy function. Is the policy generated also optimal one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['X' '←' '←' '←']\n",
      " ['↑' '↑' '←' '↓']\n",
      " ['↑' '↑' '↓' '↓']\n",
      " ['↑' '→' '→' 'X']]\n"
     ]
    }
   ],
   "source": [
    "# greedy policy\n",
    "policy = np.full_like(V, 'X', 'str')\n",
    "\n",
    "for row in range(rows_count):\n",
    "    for col in range(columns_count):\n",
    "        if (row, col) in terminating:\n",
    "            continue\n",
    "        # if rewards are the same they do not need to be taken into account while greedily choosing a policy, as:\n",
    "        # argmax([r + v1, r + v2, ..., r + vk]) = argmax([v1, v2, ..., vk])\n",
    "        direction_index = np.argmax([V[take_action(row, col, action)] for action in DIRECTIONS])\n",
    "        policy[row, col] = DIRECTIONS_LABELS[direction_index]\n",
    "\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks optimal to me :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Value function and policy in modified gridworld.\n",
    "\n",
    "Change the definition of the exercise 1 gridworld by assigning a cost of -8 to movement in \"up\" direction. Compute the value function and greedy policy based on the value function. Is the greedy policy optimal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 197\n",
      "[[  0.  -38.5 -55.  -60.5]\n",
      " [-38.5 -49.5 -55.  -55. ]\n",
      " [-55.  -55.  -49.5 -38.5]\n",
      " [-60.5 -55.  -38.5   0. ]]\n"
     ]
    }
   ],
   "source": [
    "rows_count = 4\n",
    "columns_count = 4\n",
    "V = np.zeros((rows_count, columns_count))\n",
    "V_new = np.zeros((rows_count, columns_count))\n",
    "terminating = [(0,0), (rows_count-1, columns_count-1)]\n",
    "stepcost = -1\n",
    "\n",
    "maxiters = 1000\n",
    "eps = 0.0000001\n",
    "\n",
    "def take_action(row, column, direction):\n",
    "    return tuple(np.clip((row, column) + direction, 0, 3))\n",
    "\n",
    "def get_cost(action):\n",
    "    if action is UP:\n",
    "        return -8\n",
    "    return -1\n",
    "\n",
    "for i in range(maxiters):\n",
    "    for row in range(rows_count):\n",
    "        for col in range(columns_count):\n",
    "            if (row, col) in terminating:\n",
    "                continue\n",
    "            V = V_new.copy()\n",
    "            V_new[row, col] = sum((get_cost(action) + V[take_action(row, col, action)] for action in DIRECTIONS)) / 4\n",
    "    \n",
    "    if LA.norm(V - V_new) < eps:\n",
    "        break\n",
    "\n",
    "print(\"iteration\", i)\n",
    "with np.printoptions(precision=3):\n",
    "    print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['X' '←' '←' '←']\n",
      " ['↑' '←' '←' '↓']\n",
      " ['↑' '→' '↓' '↓']\n",
      " ['→' '→' '→' 'X']]\n"
     ]
    }
   ],
   "source": [
    "# greedy policy\n",
    "policy = np.full_like(V, 'X', 'str')\n",
    "\n",
    "for row in range(rows_count):\n",
    "    for col in range(columns_count):\n",
    "        if (row, col) in terminating:\n",
    "            continue\n",
    "        direction_index = np.argmax([get_cost(action) + V[take_action(row, col, action)] for action in DIRECTIONS])\n",
    "        policy[row, col] = DIRECTIONS_LABELS[direction_index]\n",
    "\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That policy is not optimal. Proof:\n",
    "For state `(2, 3)` (one above lower left corner) the state value under the current policy (taking action `UP`) is *-16*.\n",
    "A better policy would be to take the action `DOWN` in the state, with actions for the remaining states kept the same,\n",
    "now the state value is *-4*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra exercise: Policy iteration\n",
    "\n",
    "Implement policy iteration, ie. create a policy with the help of the value function from previous policy and iterate until policy is stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. -1. -2. -3.]\n",
      " [-5. -4. -3. -2.]\n",
      " [-4. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "rows_count = 4\n",
    "columns_count = 4\n",
    "V = np.zeros((rows_count, columns_count))\n",
    "V_new = np.zeros((rows_count, columns_count))\n",
    "terminating = [(0,0), (rows_count-1, columns_count-1)]\n",
    "\n",
    "policy = np.empty_like(V, dtype=np.object)\n",
    "for i in range(len(policy.flat)):\n",
    "    # np.random.choice was reluctant to work\n",
    "    policy.flat[i] = DIRECTIONS[np.random.randint(len(DIRECTIONS))]\n",
    "for r, c in terminating:\n",
    "    policy[r,c] = None\n",
    "\n",
    "maxiters = 1000\n",
    "eps = 0.0000001\n",
    "\n",
    "def take_action(row, column, direction):\n",
    "    return tuple(np.clip((row, column) + direction, 0, 3))\n",
    "\n",
    "def get_cost(action):\n",
    "    if action is UP:\n",
    "        return -8\n",
    "    return -1\n",
    "\n",
    "for j in range(10):\n",
    "    # policy evaluation\n",
    "    for i in range(maxiters):\n",
    "        for row in range(rows_count):\n",
    "            for col in range(columns_count):\n",
    "                if (row, col) in terminating:\n",
    "                    continue\n",
    "                V = V_new.copy()\n",
    "                action = policy[row, col]\n",
    "                V_new[row, col] = get_cost(action) + V[take_action(row, col, action)]\n",
    "\n",
    "        if LA.norm(V - V_new) < eps:\n",
    "            break\n",
    "\n",
    "    # policy improvement\n",
    "    policy_stable = True\n",
    "    for row in range(rows_count):\n",
    "        for col in range(columns_count):\n",
    "            if (row, col) in terminating:\n",
    "                continue\n",
    "            old_action = policy[row, col]\n",
    "            direction_index = np.argmax([get_cost(action) + V[take_action(row, col, action)] for action in DIRECTIONS])\n",
    "            policy[row, col] = DIRECTIONS[direction_index]\n",
    "            if old_action is not policy[row, col]:\n",
    "                policy_stable = False\n",
    "            \n",
    "    if policy_stable:\n",
    "        break\n",
    "\n",
    "with np.printoptions(precision=3):\n",
    "    print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['X' '←' '←' '↓']\n",
      " ['↓' '↓' '↓' '↓']\n",
      " ['↓' '↓' '↓' '↓']\n",
      " ['→' '→' '→' 'X']]\n"
     ]
    }
   ],
   "source": [
    "# policy to arrows\n",
    "pretty_policy = np.full_like(policy, 'X', 'str')\n",
    "\n",
    "for row in range(rows_count):\n",
    "    for col in range(columns_count):\n",
    "        if (row, col) in terminating:\n",
    "            continue\n",
    "        # direction_index = DIRECTIONS.index(policy[row, col]) # this has problems with numpy\n",
    "        direction_index = np.argmax([policy[row, col] is d for d in DIRECTIONS])\n",
    "        pretty_policy[row, col] = DIRECTIONS_LABELS[direction_index]\n",
    "\n",
    "print(pretty_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is indeed optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Value iteration\n",
    "\n",
    "Solve the exercise 1 gridworld with value iteration algorithm. Solve also modified gridworld (cost of \"up\" movement = -4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# value iteration, reward = -1\n",
    "theta = 0.0001\n",
    "maxiters = 10\n",
    "V = np.zeros((rows_count,columns_count))\n",
    "\n",
    "def take_action(row, column, direction):\n",
    "    return tuple(np.clip((row, column) + direction, 0, 3))\n",
    "\n",
    "for i in range(maxiters):\n",
    "    delta = 0\n",
    "    for row in range(rows_count):\n",
    "        for col in range(columns_count):\n",
    "            if (row, col) in terminating:\n",
    "                continue\n",
    "            # the asynchronous would not be working the same way it did in the previous exercises\n",
    "            # synchronous would work the same, but I was mood for something fresh from the book\n",
    "            v = V[row, col]\n",
    "            V[row, col] = max((-1 + V[take_action(row, col, action)] for action in DIRECTIONS))\n",
    "            delta = max(delta, abs(v - V[row, col]))\n",
    "    \n",
    "    if delta < theta:\n",
    "        break\n",
    "\n",
    "with np.printoptions(precision=3):\n",
    "    print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['X' '←' '←' '↓']\n",
      " ['↑' '↑' '↑' '↓']\n",
      " ['↑' '↑' '↓' '↓']\n",
      " ['↑' '→' '→' 'X']]\n"
     ]
    }
   ],
   "source": [
    "# greedy policy\n",
    "policy = np.full_like(V, 'X', 'str')\n",
    "\n",
    "for row in range(rows_count):\n",
    "    for col in range(columns_count):\n",
    "        if (row, col) in terminating:\n",
    "            continue\n",
    "        direction_index = np.argmax([-1 + V[take_action(row, col, action)] for action in DIRECTIONS])\n",
    "        policy[row, col] = DIRECTIONS_LABELS[direction_index]\n",
    "\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again an optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. -1. -2. -3.]\n",
      " [-4. -4. -3. -2.]\n",
      " [-4. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# value iteration with modified cost of going up (-4)\n",
    "theta = 0.0001\n",
    "maxiters = 10\n",
    "V = np.zeros((rows_count,columns_count))\n",
    "\n",
    "def take_action(row, column, direction):\n",
    "    return tuple(np.clip((row, column) + direction, 0, 3))\n",
    "\n",
    "def get_cost(action):\n",
    "    if action is UP:\n",
    "        return -4\n",
    "    return -1\n",
    "\n",
    "for i in range(maxiters):\n",
    "    delta = 0\n",
    "    for row in range(rows_count):\n",
    "        for col in range(columns_count):\n",
    "            if (row, col) in terminating:\n",
    "                continue\n",
    "            v = V[row, col]\n",
    "            V[row, col] = max((get_cost(action) + V[take_action(row, col, action)] for action in DIRECTIONS))\n",
    "            delta = max(delta, abs(v - V[row, col]))\n",
    "    \n",
    "    if delta < theta:\n",
    "        break\n",
    "\n",
    "with np.printoptions(precision=3):\n",
    "    print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['X' '←' '←' '↓']\n",
      " ['↑' '↓' '↓' '↓']\n",
      " ['↓' '↓' '↓' '↓']\n",
      " ['→' '→' '→' 'X']]\n"
     ]
    }
   ],
   "source": [
    "# greedy policy\n",
    "policy = np.full_like(V, 'X', 'str')\n",
    "\n",
    "for row in range(rows_count):\n",
    "    for col in range(columns_count):\n",
    "        if (row, col) in terminating:\n",
    "            continue\n",
    "        direction_index = np.argmax([get_cost(action) + V[take_action(row, col, action)] for action in DIRECTIONS])\n",
    "        policy[row, col] = DIRECTIONS_LABELS[direction_index]\n",
    "\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal (by eyeballing...). The value iteration is really neat and beautiful for this problem!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
